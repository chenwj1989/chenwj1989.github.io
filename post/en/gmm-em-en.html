<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="/static/img/favicon.ico" />
    <title>Gaussian Mixture Model and  Expectation-Maximization Algorithm - All Articles</title>
    <meta name="author" content="wjchen" />
    <meta name="description" content="Gaussian Mixture Model and  Expectation-Maximization Algorithm" />
    <meta name="keywords" content="Gaussian Mixture Model and  Expectation-Maximization Algorithm, All Articles, machine_learning" />
    <link rel="alternate" type="application/rss+xml" title="RSS" href="/feed.xml">
    <meta content="" property="fb:app_id">
    <meta content="All Articles" property="og:site_name">

    

    
      <meta content="Gaussian Mixture Model and  Expectation-Maximization Algorithm" property="og:title">
      <meta content="article" property="og:type">
    

    
      <meta content="My Personal Thoughts" property="og:description">
    

    
      <meta content="https://chenwj1989.github.io/post/en/gmm-em-en.html" property="og:url">
    

    
      <meta content="2019-06-18T14:32:04+08:00" property="article:published_time">
      <meta content="https://chenwj1989.github.io/about/" property="article:author">
    

    
      <meta content="https://chenwj1989.github.io/static/img/avatar.jpg" property="og:image">
    

    
      
        <meta content="machine_learning" property="article:section">
      
    

    
      
    

    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@">
    <meta name="twitter:creator" content="@">

    
      <meta name="twitter:title" content="Gaussian Mixture Model and  Expectation-Maximization Algorithm">
    

    
      <meta name="twitter:url" content="https://chenwj1989.github.io/post/en/gmm-em-en.html">
    

    
      <meta name="twitter:description" content="My Personal Thoughts">
    

    

    <!-- Font awesome icons -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">
    <!-- syntax highlighting CSS -->
    <link rel="stylesheet" href="/static/css/syntax.css">
    <!-- Bootstrap core CSS -->
    <link href="/static/css/bootstrap.min.css" rel="stylesheet">
    <!-- Fonts -->
    <link href="/static/css/fonts.css" rel="stylesheet" type="text/css">
    <!-- Custom CSS -->
    <link rel="stylesheet" href="/static/css/super-search.css">
    <link rel="stylesheet" href="/static/css/thickbox.css">
    <link rel="stylesheet" href="/static/css/projects.css">
    <link rel="stylesheet" href="/static/css/main.css">
    <link rel="stylesheet" href="/static/katex/katex.min.css">

    
  </head>
  <body>
    <div class="container">
      <div class="col-sm-3">
        <div class="fixed-condition">
          <div class="author" align="center">
          <a href="/"><img class="profile-avatar" src="/static/img/avatar.jpg" height="100px" width="100px" /></a>
          <h1 class="author-name">wjchen</h1>
          
            <div class="profile-about">
              Never lose a holy curiosity.
            </div>
          
           <div class="social">
            <ul>
              
                <li><a href="https://github.com/chenwj1989" target="_blank"><i class="fab fa-github"></i></a></li>
              
                <li><a href="https://www.zhihu.com/people/wjchen-16/activities" target="_blank"><i class="fab fa-zhihu"></i></a></li>
              
            </ul>
          </div>
	  </div>
          <div class="search" id="js-search">
            <input type="text" placeholder="$ type to search" class="search__input form-control" id="js-search__input">
            <ul class="search__results" id="js-search__results"></ul>
          </div>
          <hr />
          <ul class="sidebar-nav">
            <strong>Navigation</strong>
            <li><a href="/">Home</a></li>
            
              <li><a class="about" href="/projects/">My Projects</a></li>
            
              <li><a class="about" href="/about/">About Me</a></li>
            
          </ul>
        </div>
        <!-- end /.fixed-condition -->
      </div>
      <div class="col-sm-8 col-offset-1 main-layout">
        <header class="post-header">
  <h1 class="post-title">Gaussian Mixture Model and  Expectation-Maximization Algorithm</h1>
</header>

<span class="time">18 Jun 2019</span>

  <span class="categories">
    &raquo; <a href="/category/machine_learning">machine_learning</a>
  </span>


<div class="content">
  <div class="post"><p><br /></p>
<ul id="markdown-toc">
  <li><a href="#1-preliminary-topics" id="markdown-toc-1-preliminary-topics">1. Preliminary Topics</a>    <ul>
      <li><a href="#11-gaussian-distribution" id="markdown-toc-11-gaussian-distribution">1.1 Gaussian Distribution</a></li>
      <li><a href="#12-jensens-inequality" id="markdown-toc-12-jensens-inequality">1.2 Jensen’s Inequality</a></li>
      <li><a href="#13-matrix-derivatives" id="markdown-toc-13-matrix-derivatives">1.3 Matrix Derivatives</a></li>
    </ul>
  </li>
  <li><a href="#2gaussian-mixture-model-gmm-and-expectation-maximizationem-algorithm" id="markdown-toc-2gaussian-mixture-model-gmm-and-expectation-maximizationem-algorithm">2.Gaussian Mixture Model (GMM) and Expectation-Maximization(EM) Algorithm</a>    <ul>
      <li><a href="#21-gmm" id="markdown-toc-21-gmm">2.1 GMM</a></li>
      <li><a href="#22-em" id="markdown-toc-22-em">2.2 EM</a></li>
    </ul>
  </li>
  <li><a href="#3em-algorithm-for-univariate-gmm" id="markdown-toc-3em-algorithm-for-univariate-gmm">3.EM Algorithm for Univariate GMM</a>    <ul>
      <li><a href="#31-e-step" id="markdown-toc-31-e-step">3.1 E-Step:</a></li>
      <li><a href="#32-m-step" id="markdown-toc-32-m-step">3.2 M-Step:</a></li>
    </ul>
  </li>
  <li><a href="#4em-algorithm-for-multivariate-gmm" id="markdown-toc-4em-algorithm-for-multivariate-gmm">4.EM Algorithm for Multivariate GMM</a>    <ul>
      <li><a href="#41-e-step" id="markdown-toc-41-e-step">4.1 E-Step:</a></li>
      <li><a href="#42-m-step" id="markdown-toc-42-m-step">4.2 M-Step:</a></li>
    </ul>
  </li>
  <li><a href="#5summary" id="markdown-toc-5summary">5.Summary</a></li>
</ul>

<h2 id="1-preliminary-topics">1. Preliminary Topics</h2>

<h3 id="11-gaussian-distribution">1.1 Gaussian Distribution</h3>

<p>The Gaussian distribution is very widely used to fit random data. The
probability density for a one-dimensional random variable $x$ follows:</p>

<script type="math/tex; mode=display">\textit{N}(x; \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right]</script>

<p>where</p>
<ul>
  <li>
    <p>$\mu $ is the mean or expectation of the distribution,</p>
  </li>
  <li>
    <p>$\sigma$ is the standard deviation, and $ \sigma ^{2}$ is the
variance.</p>
  </li>
</ul>

<p>More generally, when the data set is a d-dimensional data, it can be fit by a multivariate Gaussian model. The probability density is:</p>

<script type="math/tex; mode=display">\textit{N}(x; \mu, \Sigma) = \frac{1}{\sqrt{(2\pi)^d\det(\Sigma)}}\exp\left[-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right]</script>

<p>where</p>
<ul>
  <li>$x$ is a d-by-N vector, representing N sets of d-dimensional random data,</li>
  <li>$\mu$ is a d-by-1 vector, representing the mean of each dimension,</li>
  <li>$\Sigma$ is a d-by-d matrix, representing the covariance matrix</li>
</ul>

<h3 id="12-jensens-inequality">1.2 Jensen’s Inequality</h3>

<p>Here statements of Jensen’s inequality in the context of probability theory. These would be used to simplify the target function in an EM process.</p>

<p><strong>Theorem.</strong>For convex function $f$ and a random variable $x$:</p>

<script type="math/tex; mode=display">f\left[E(x)\right] \leq E\left[f(x)\right]</script>

<figure align="center" style="width: 60%;margin:auto">
  <img width="70%" height="70%" src="/static/posts/convex.png" />
  <figcaption>Fig.1 - An example of a convex function. Let $x$ be evenly distributed between
a and b. The expectation of $f(x)$ is always above $f[E(x)]$.</figcaption>
</figure>
<p><br />
<strong>Theorem.</strong> For concave function $f$ and a random variable $x$:</p>

<script type="math/tex; mode=display">f\left[E(x)\right] \geq E\left[f(x)\right]</script>

<figure align="center" style="width: 60%;margin:auto">
  <img width="70%" height="70%" src="/static/posts/concave.png" />
  <figcaption>Fig.2 - An example of a concave function. Let $x$ be evenly distributed
between a and b. The expectation of $f(x)$ is always under $f[E(x)]$.</figcaption>
</figure>

<h3 id="13-matrix-derivatives">1.3 Matrix Derivatives</h3>

<p>In order to solve the parameters in a Gaussian mixture model, we need
some rules about derivatives of a matrix or a vector. Here are some 
useful equations cited from <em>The Matrix Cookbook</em>.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\frac{\partial x^Ta}{\partial x} &= \frac{\partial a^Tx}{\partial x} = a\\
\frac{\partial x^TBx}{\partial x} &=  (B + B^T )x\\
\frac{\partial (x -s)^TW(x-s)}{\partial x} &= -2W(x-s), \text{ (W is symmetric)} \\
\frac{\partial a^TXa}{\partial X} &= \frac{\partial a^TX^Ta}{\partial X} = aa^T\\
\frac{\partial \det(X)}{\partial X} &= \det(X)(X^{-1})^T\\
\frac{\partial \ln \det(X)}{\partial X} &= (X^{-1})^T\\
\frac{\partial \det(X^{-1})}{\partial X} &= -\det(X^{-1})(X^{-1})^T\\
\frac{\partial \ln \det(X)}{\partial X^{-1}} 
&= \frac{\partial \ln \det(X)}{\partial \det(X)}\frac{\partial \det(X)}{\partial X^{-1}} \\
&= \frac{1}{\det(X)}\left[-\det(X)X^T\right]\\
&= -X^T\\
\frac{\partial Tr(AXB)}{\partial X} &= A^TB^T\\
\frac{\partial Tr(AX^-1B)}{\partial X} &= -(X^{-1}BAX^{-1})^T\\
\end{aligned} %]]></script>

<h2 id="2gaussian-mixture-model-gmm-and-expectation-maximizationem-algorithm">2.Gaussian Mixture Model (GMM) and Expectation-Maximization(EM) Algorithm</h2>

<h3 id="21-gmm">2.1 GMM</h3>

<p>For a complex data set in the real-world, it normally consists of a
mixture of multiple stochastic processes. Therefore a single Gaussian
distribution cannot fit such data set. Instead, a Gaussian mixture model
is used to describe a combination of $K$ Gaussian distribution.</p>

<p>Suppose we have a training set of $N$ independent data points $x = {x_1, x_2 …x_i… x_N}$, and the values show multiple peaks. We can model this data set by a Gaussian mixture model</p>

<script type="math/tex; mode=display">p(x|\Theta) = \sum_{k}^{}\alpha_{k}\textit{N}(x; \mu_k, \sigma_k) = \sum_{k}^{}\alpha_{k}\frac{1}{\sqrt{2\pi\sigma_k^2}}\exp\left[-\frac{(x-\mu_k)^2}{2\sigma_k^2}\right]</script>

<table style="width: 600px; height: 200px; margin:auto">
    <tr>
        <td style="text-align:center">
	<img width="90%" height="auto" src="/static/posts/uniGMM.png" />
	</td>
        <td style="text-align:center">
	<img width="90%" height="auto" src="/static/posts/multiGMM.png" />
	</td>
    </tr>
    <tr>
        <td style="text-align:center" valign="top">
  	<figcaption>Fig.3 - Probability density of a univariate GMM with K=2 </figcaption>
	</td>
        <td style="text-align:center" valign="top">
  	<figcaption>Fig.4 - Samples of a 2d GMM with K=2</figcaption>
	</td>
    </tr>
</table>
<p><br />
When each data sample $x_i$ is d-dimensional, and the data set $x$ seem scattering to multiple clusters, the data can be modeled by a multivariate version Gaussian mixture model.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
p(x|\Theta) &= \sum_{k}^{}\alpha_{k}\textit{N}(x; \mu_k, \Sigma_k)  \\
&= \sum_{k}^{}\alpha_{k}\frac{1}{\sqrt{(2\pi)^d\det(\Sigma_k)}}\exp\left[-\frac{1}{2}(x-\mu_k)^T\Sigma_{k}^{-1}(x-\mu_k)\right]
\end{aligned} %]]></script>

<p>In the models,  $\Theta$ means all parameters, and $\alpha_k$ is the prior probability of th $k^{th}$ Gaussian model, and</p>

<script type="math/tex; mode=display">\sum_{k}\alpha_k = 1</script>

<h3 id="22-em">2.2 EM</h3>
<p>The expectation-maximization(EM) algorithm is an iterative supervised training algorithm. The task is formulated as:</p>
<ul>
  <li>We have a training set of $N$ independent data points $x$.</li>
  <li>Either we know or have a good guess, that the data set is a mixture
of $K$ Gaussian distributions.</li>
  <li>The task is to estimate the GMM parameters: K set of ($\alpha_{k}$,
$\mu_k$, $\sigma_k$), or ($\alpha_{k}$, $\mu_k$, $\Sigma_{k}$).</li>
</ul>

<p><strong>Likelihood Function:</strong></p>

<p>For estimation problems based on data set of independent samples, maximum-likelihood estimation (MLE) is a very widely used and straight forward method to perform estimation.</p>

<p>The probability of N independent tests is described as the product of probability of each test. This is called the likelihood function:</p>

<script type="math/tex; mode=display">p(x|\Theta) = \prod_{i}p(x_i|\Theta)</script>

<p>MLE is to estimate the  parameters $\Theta$ by maximizing the likelihood function.</p>

<script type="math/tex; mode=display">\Theta = \argmax_{\Theta} \prod_{i}p(x_i|\Theta)</script>

<p>By applying the MLE , the likelihood function for uni and multiple variate Gaussian mixture models are very complicated.</p>

<script type="math/tex; mode=display">p(x|\Theta) = \prod_{i}p(x_i|\Theta) = \prod_{i}\left[\sum_{k}\alpha_{k}\textit{N}(x_i| \mu_k, \sigma_k) \right]</script>

<script type="math/tex; mode=display">p(x|\Theta) = \prod_{i}p(x_i|\Theta) = \prod_{i}\left[\sum_{k}\alpha_{k}\textit{N}(x_i| \mu_k, \Sigma_k)\right]</script>

<p>To estimate K set of Gaussian parameters directly and explicitly is difficult. The EM algorithm simplifies the likelihood function of GMM, and provides an iterative way to optimize the estimation.Here we try to briefly describe the EM algorithm for GMM parameter estimation.</p>

<p>First, the likelihood function of a GMM model can be simplified by taking the log likelihood function. An formula with the form of summation is easier for separating independent data samples and taking derivatives of parameters.</p>

<script type="math/tex; mode=display">L(x|\Theta) = \sum_{i}\ln\left[p(x_{i}|\mu_k, \sigma_k) \right] = \sum_{i}\ln\left[\sum_{k}\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k) \right]</script>

<p><strong>Latent Parameters:</strong></p>

<p>There is no efficient way to explicitly maximizing the log likelihood function for GMM above. The EM algorithm introduces a latent parameter $z$, that $z \in {1 ,2 … k … K}$. That is used to describe the <strong>probability of a given training sample $x_i$ belonging to cluster z, given full GMM parameters</strong>:
<script type="math/tex">p(z|x_{i}, \mu_k, \sigma_k)</script></p>

<p>Introduce the latent parameter $z$ in the  probability distribution of $x_i$.</p>

<script type="math/tex; mode=display">p(x_{i}|\Theta) = \sum_{k} p(x_{i}|z=k,\mu_k, \sigma_k) p(z=k) \\</script>

<p>Compared with 
$p(x|\Theta) = \sum_{k}^{}\alpha_{k}\textit{N}(x|\mu_k, \sigma_k)$,</p>

<p>we can conclude that $\alpha_k \text{ is the prior probability of } p(z=k)$.</p>

<script type="math/tex; mode=display">p(z=k) = \alpha_k</script>

<p>and the conditional probability of $x$ given $z=k$ is the $k^{th}$ Gaussian model.</p>

<script type="math/tex; mode=display">p(x_{i}|z=k,\mu_k, \sigma_k)  = \textit{N}(x_i; \mu_k, \sigma_k)</script>

<p>Now the latent parameter can be introduced into the log likelihood
function. Be noted that an redundant term $p(z|x_{i},\mu_k, \sigma_k) $
is added, in order to match the form of Jensen’s inequality.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
L(x|\Theta) &= \sum_{i}\ln\left[p(x_{i}, z|\mu_k, \sigma_k) \right]  \\
&= \sum_{i}\ln \sum_{k}  p(x_{i}|z=k, \mu_k, \sigma_k)p(z=k)   \\
&= \sum_{i}\ln \sum_{k} p(z=k|x_{i},\mu_k, \sigma_k) \frac{p(x_{i}|z=k, \mu_k, \sigma_k)p(z=k)}{p(z=k|x_{i},\mu_k, \sigma_k)} \\
\end{aligned} %]]></script>

<p><strong>Simplify the Likelihood function:</strong></p>

<p>However the summation inside a log function make it difficult to maximize. Here recall Jensen’s inequality:</p>

<script type="math/tex; mode=display">f\left[E(x)\right] \geq E\left[f(x)\right]</script>

<p>Let $u$ represent 
$\frac{p(x_{i} | z=k, \mu_k, \sigma_k)p(z=k)}{p(z | x_{i},\mu_k, \sigma_k)}$ to match Jensen’s inequality.</p>

<p>We get</p>

<script type="math/tex; mode=display">f(u) = \ln u</script>

<script type="math/tex; mode=display">E(u) = \sum_{k} p(z|x_{i},\mu_k, \sigma_k) u</script>

<p>Therefore,</p>

<script type="math/tex; mode=display">L(x|\Theta) \geq \sum_{i}\sum_{k} p(z=k|x_{i},\mu_k, \sigma_k) \ln \frac{p(x_{i}|z=k, \mu_k, \sigma_k)p(z=k)}{p(z=k|x_{i},\mu_k, \sigma_k)}</script>

<p>The posterior probability can be derived by the Bayes’ law.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
p(z=k|x_{i},\mu_k, \sigma_k) &= \frac{ p(x_{i}|z=k,\mu_k, \sigma_k)}{ \sum_{k} p(x_{i}|z=k, \mu_k, \sigma_k)}  \\
&=  \frac{\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}{\sum_{k}\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)} 
\end{aligned} %]]></script>

<p>Define
<script type="math/tex">\omega_{i,k} = p(z=k|x_{i},\mu_k, \sigma_k) = \frac{\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}{\sum_{k}\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}</script></p>

<p>Then</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
L(x|\Theta) &= \sum_{i}\ln \sum_{k} \omega_{i,k} \frac{\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}{\omega_{i,k}}  \\
&\geq \sum_{i} \sum_{k} \omega_{i,k} \ln\frac{\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}{\omega_{i,k}} 
\end{aligned} %]]></script>

<p>This equation defines a lower bound for the log likelihood function. Therefore, an iterative target function for the EM algorithm is defined:</p>

<script type="math/tex; mode=display">Q(\Theta,\Theta^{t}) = \sum_{i}\sum_{k}\omega_{i,k}^t \ln\frac{\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}{\omega_{i,k}^t}</script>

<p>After $t$ iterations, we’ve got $\Theta^t$，and hence the latent $\omega_{i,k}^t$。 Apply the latest latent parameters in $Q(\Theta,\Theta^{t})$，and then we can update $\Theta^{t+1}$ by maiximization.</p>

<p><strong>Iterative Optimization:</strong> </p>

<p>First the parameters $\Theta$ are initialized, and then $\omega$ and $\Theta$ are updated iteratively.</p>
<ul>
  <li>
    <p>After iteration t, a set of parameters $\Theta^t$ have been
achieved.</p>
  </li>
  <li>
    <p>Calculate latent parameters $\omega_{i,k}^t$ by applying $\Theta^t$
into the GMM. This step is called <strong>expectation step</strong>.</p>

    <script type="math/tex; mode=display">\omega_{i,k}^t = \frac{\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}{\sum_{k}\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}</script>
  </li>
  <li>
    <p>Apply the latest latent parameters $\omega_{i,k}^t$ in the target function. The target function is derived by simplifying the log likelihood funciton by Jensen’s inequality.</p>

    <script type="math/tex; mode=display">Q(\Theta,\Theta^{t}) = \sum_{i}\sum_{k}\omega_{i,k}^t \ln\frac{\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}{\omega_{i,k}^t}</script>
  </li>
  <li>
    <p>With $\omega_{i,k}$, maximize the target log likelihood function, to
update GMM parameters $\Theta^{t+1}$. This step is called
<strong>maximization step</strong>.</p>

    <script type="math/tex; mode=display">\Theta^{t+1} = \argmax_{\Theta} \sum_{i}\sum_{k} \ln \omega_{i,k}^t \frac{\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}{\omega_{i,k}^t}</script>
  </li>
</ul>

<h2 id="3em-algorithm-for-univariate-gmm">3.EM Algorithm for Univariate GMM</h2>

<p>The complete form of the EM target function for a univariate GMM is</p>

<script type="math/tex; mode=display">Q(\Theta,\Theta^{t}) = \sum_{i}\sum_{k}\omega_{i,k}^t\ln\frac{\alpha_{k}}{\omega_{i,k}^t\sqrt{2\pi\sigma_k^2}}\exp\left[-\frac{(x_i-\mu_k)^2}{2\sigma_k^2}\right]</script>

<h3 id="31-e-step">3.1 E-Step:</h3>

<p>The E-step is to estimate the latent parameters for each training sample on K Gaussian models. Hence the latent parameter $\omega$ is a N-by-K matrix.</p>

<p>On every iteration, $\omega_{i,k}$ is calculated from the latest Gaussian parameters $(\alpha_k, \mu_k, \sigma_k)$</p>

<script type="math/tex; mode=display">\omega_{i,k}^t = \frac{\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \sigma_k^t)}{\sum_{k}\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \sigma_k^t)}</script>

<h3 id="32-m-step">3.2 M-Step:</h3>

<script type="math/tex; mode=display">\Theta := \argmax_{\Theta} Q(\Theta,\Theta^{t})</script>

<p>The target likelihood function can be expanded to decouple items clearly.</p>

<script type="math/tex; mode=display">Q(\Theta,\Theta^{t}) = \sum_{i}\sum_{k}\omega_{i,k}^t\left(\ln\alpha_k - \ln \omega_{i,k}^t - \ln \sqrt{2\pi\sigma_k^2}-\frac{(x_i-\mu_k)^2}{2\sigma_k^2}\right)</script>

<p><strong>Update $\alpha_k:$</strong></p>

<p>As defined in GMM, $\alpha_k$ is constrained by $\sum_{k}\alpha_k =1$, so estimating $\alpha_k$ is a constrained optimization problem.</p>

<script type="math/tex; mode=display">\begin{gathered}
\alpha_k^{t+1} := \argmax_{\alpha_k}{ \sum_{i}\sum_{k}\omega_{i,k}^t\ln\alpha_k}\\
\text{subject to}  \sum_{k}\alpha_k =1 
\end{gathered}</script>

<p>The method of Lagrange multipliers is used to find the local maxima of such constrained optimization problem. We can construct a Lagrangean function:</p>

<script type="math/tex; mode=display">\mathcal{L}(\alpha_k, \lambda) = { \sum_{i}\sum_{k}\omega_{i,k}^t\ln\alpha_k}+ \lambda\left[\sum_{k}\alpha_k -1\right]</script>

<p>The local maxima $\alpha_{k}^{t+1}$ should make the derivative of the Lagrangean function equal to 0. Hence,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\frac{\partial \mathcal{L}(\alpha_k, \lambda) }{\partial \alpha_k} &= { \sum_{i}\omega_{i,k}^t\frac{1}{\alpha_k}}+ \lambda = 0 \\
\Rightarrow  \alpha_k &= -\frac{\sum_{i}\omega_{i,k}^t}{\lambda}
\end{aligned} %]]></script>

<p>By summing the equation for all $k$, the value of $\lambda$ can be calculated.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\sum_{k}\alpha_k &= -\frac{\sum_{i}\sum_{k}\omega_{i,k}^t}{\lambda} \\
\Rightarrow 1 &= -\sum_{i}\frac{1}{\lambda} = -\frac{N}{\lambda}  \\
\Rightarrow \lambda &= -N  
\end{aligned} %]]></script>

<p>Therefore, $\alpha_k$ on iteration $t+1$ based on latent parameters on iteration $t$ is updated by</p>

<script type="math/tex; mode=display">\alpha_k^{t+1} = \frac{\sum_{i}\omega_{i,k}^t}{N}</script>

<p><strong>Update $\mu_k:$</strong> </p>

<p>$\mu_k$ is unconstrained, and can be derived by taking the derivative of the target likelihood function.</p>

<script type="math/tex; mode=display">\mu_k^{t+1} :=  \argmax_{\mu_k}   Q(\Theta,\Theta^{t})</script>

<p>Let $\frac{\partial Q(\Theta,\Theta^{t})}{\partial \mu_k}=0$, hence</p>

<script type="math/tex; mode=display">\frac{\partial \sum_{i}\sum_{k}\omega_{i,k}^t\left(\ln\alpha_k - \ln \omega_{i,k}^t - \ln \sqrt{2\pi\sigma_k^2}-\frac{(x_i-\mu_k)^2}{2\sigma_k^2}\right)}{\partial \mu_k} =  0</script>

<script type="math/tex; mode=display">\begin{aligned}
\sum_{i}\omega_{i,k}^t\frac{x_i-\mu_k}{\sigma_k^2} = 0\\
\Rightarrow \sum_{i}\omega_{i,k}^t\mu_k = \sum_{i}\omega_{i,k}^tx_i \\
\Rightarrow \mu_k\sum_{i}\omega_{i,k}^t = \sum_{i}\omega_{i,k}^tx_i 
\end{aligned}</script>

<p>Hence $\mu_k$ on iteration $t+1$ can be updated as a form of weighted mean of $x$.</p>

<script type="math/tex; mode=display">\mu_k^{t+1} = \frac{\sum_{i}\omega_{i,k}^tx_i}{\sum_{i}\omega_{i,k}^t}</script>

<p><strong>Update $\sigma_k:$</strong> </p>

<p>Similarly, updated $\sigma_k$ is derived by taking the derivative of the target likelihood function with respect to $\sigma_k$.</p>

<script type="math/tex; mode=display">\sigma_k^{t+1} :=  \argmax_{\sigma_k}   Q(\Theta,\Theta^{t})</script>

<p>Let</p>

<p><script type="math/tex">\frac{\partial Q(\Theta,\Theta^{t})}{\partial \sigma_k} = \frac{\partial \sum_{i}\sum_{k}\omega_{i,k}^t\left(\ln\alpha_k - \ln \omega_{i,k}^t - \ln \sqrt{2\pi\sigma_k^2}-\frac{(x_i-\mu_k)^2}{2\sigma_k^2}\right)}{\partial \sigma_k}=0</script>.</p>

<p>We get</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
 \sum_{i}\omega_{i,k}\left[-\frac{1}{\sigma_k}+\frac{(x_i-\mu_k)^2}{\sigma_k^3}\right]&= 0\\
\Rightarrow \sum_{i}\omega_{i,k}\sigma_k^2 &= \sum_{i}\omega_{i,k}(x_i-\mu_k)^2 \\
\Rightarrow \sigma_k^2 \sum_{i}\omega_{i,k} &= \sum_{i}\omega_{i,k}(x_i-\mu_k)^2 \\
\end{aligned} %]]></script>

<p>For $\sigma_k$, we can update $\sigma_k^2$, which is enough for Gaussian model calculation. New $sigma_k^2$ depends on $\mu_k$, so normally $\mu_k^{t+1}$ is calculated first and then applied to the update equation for $\sigma_k^2$</p>

<script type="math/tex; mode=display">(\sigma_k^2)^{t+1} = \frac{\sum_{i}\omega_{i,k}(x_i-\mu_k^{t+1})^2 }{\sum_{i}\omega_{i,k}}</script>

<h2 id="4em-algorithm-for-multivariate-gmm">4.EM Algorithm for Multivariate GMM</h2>

<p>Similarlyt the target likelihood function for a multivariat GMM is</p>

<script type="math/tex; mode=display">Q(\Theta,\Theta^{t}) = \sum_{i}\sum_{k}\omega_{i,k}^t\ln\frac{\alpha_{k}}{\omega_{i,k}^t\sqrt{(2\pi)^d\det(\Sigma_k)}}\exp\left[-\frac{1}{2}(x_i-\mu_k)^T\Sigma_k^{-1}(x_i-\mu_k)\right]</script>

<p>Be aware that</p>
<ul>
  <li>$x_i$ is a d-by-1 vecotr,</li>
  <li>$\alpha_k$ is a real number between [0,1],</li>
  <li>$\mu_k$ is a d-by-1 vector,</li>
  <li>$\Sigma_k$ is a d-by-d matrix.</li>
  <li>$\omega$ is a N-by-K matrix.</li>
</ul>

<h3 id="41-e-step">4.1 E-Step:</h3>

<p>The E-step to estimate the latent parameters is the same as univariate GMM, except that the Gaussian distribution is a multivariate one, which is more complicated.</p>

<script type="math/tex; mode=display">\omega_{i,k}^t = \frac{\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \Sigma_k^t)}{\sum_{k}\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \Sigma_k^t)}</script>

<p>The target likelihood function can be expanded.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
&Q(\Theta,\Theta^{t}) \\
&= \sum_{i}\sum_{k}\omega_{i,k}^t\left(\ln\alpha_k - \ln \omega_{i,k}^t - \frac{d}{2}\ln \sqrt{(2\pi)^d} -\frac{1}{2}\ln\det(\Sigma_k)-\frac{1}{2}(x_i-\mu_k)^T\Sigma_k^{-1}(x_i-\mu_k)\right) 
\end{aligned} %]]></script>

<h3 id="42-m-step">4.2 M-Step:</h3>

<p><strong>Update $\alpha_{k}:$</strong></p>

<p>The formula to update $\alpha_k$ for multivariate GMMs is exactly the same as univariate GMMs.</p>

<script type="math/tex; mode=display">\begin{gathered}
\alpha_k^{t+1} := \argmax_{\alpha_k}{ \sum_{i}\sum_{k}\omega_{i,k}^t\ln\alpha_k}\\
\text{subject to}  \sum_{k}\alpha_k =1 
\end{gathered}</script>

<p>Hence we get the same update equation.</p>

<script type="math/tex; mode=display">\alpha_k^{t+1} = \frac{\sum_{i}\omega_{i,k}^t}{N}</script>

<p><strong>Update $\mu_k:$</strong></p>

<script type="math/tex; mode=display">\mu_k^{t+1} :=  \argmax_{\mu_k}   Q(\Theta,\Theta^{t})</script>

<p>Take the derivative of $Q(\Theta,\Theta^{t})$ with respec to $\mu_k$, we get</p>

<script type="math/tex; mode=display">\frac{\partial Q(\Theta,\Theta^{t})}{\partial \mu_k} = \sum_{i}\omega_{i,k}^t\frac{\partial \left[-\frac{1}{2}(x_i-\mu_k)^T\Sigma_k^{-1}(x_i-\mu_k)\right]}{\partial \mu_k}  = 0\\</script>

<p>As the covariance matrix $\Sigma_{k}$ is symmetric, the inverse of it is also symmetric.  We can apply $\frac{\partial (x -s)^TW(x-s)}{\partial x} = -2W(x-s)$ (see first section) to the partial derivative.</p>

<script type="math/tex; mode=display">\frac{\partial Q(\Theta,\Theta^{t})}{\partial \mu_k} = 
 \sum_{i}\omega_{i,k}^t \Sigma_{k}^{-1}\left(x_i - \mu_k\right) =0</script>

<script type="math/tex; mode=display">\Rightarrow \sum_{i}\omega_{i,k}^t x_i= \mu_k\sum_{i}\omega_{i,k}^t</script>

<p>Hence $\mu_k$ on iteration $t+1$ is also updated as a form of weighted mean of $x$. However, in this scenario $\mu_k$ is a d-by-1 vector.</p>

<script type="math/tex; mode=display">\mu_k^{t+1} = \frac{\sum_{i}\omega_{i,k}^t x_i}{\sum_{i}\omega_{i,k}^t}</script>

<p><strong>Update $\Sigma_k:$</strong></p>

<script type="math/tex; mode=display">\Sigma_k^{t+1} :=  \argmax_{\Sigma_k}   Q(\Theta,\Theta^{t})</script>

<p>Let $\frac{\partial Q(\Theta,\Theta^{t})}{\partial \Sigma_k^{-1}} =0$, we get</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\frac{\partial Q(\Theta,\Theta^{t})}{\partial \Sigma_k^{-1}} &= \sum_{i}\omega_{i,k}^t\frac{\partial \left[  -\frac{1}{2}\ln\det(\Sigma_k)-\frac{1}{2}(x_i-\mu_k)^T\Sigma_k^{-1}(x_i-\mu_k)\right]}{\partial  \Sigma_k^{-1}}  \\
 &= -\frac{1}{2} \sum_{i}\omega_{i,k}^t \left[\frac{\partial \ln\det(\Sigma_k)}{\partial  \Sigma_k^{-1}}+\frac{\partial (x_i-\mu_k)^T\Sigma_k^{-1}(x_i-\mu_k)}{\partial  \Sigma_k^{-1}} \right] \\
 &= 0 
\end{aligned} %]]></script>

<p>By employing $\frac{\partial \ln \det(X)}{\partial X^{-1}} =-X^T $ and $\frac{\partial a^TXa}{\partial X} = aa^T$(see section one) for the symmetric covariance matrix $\Sigma_k$, and find the maxima of $ Q(\Theta,\Theta^{t})$.</p>

<script type="math/tex; mode=display">\frac{\partial Q(\Theta,\Theta^{t})}{\partial \Sigma_k^{-1}} =   \frac{1}{2}\sum_{i}\omega_{i,k}^t \left[\Sigma_k - (x_i-\mu_k)(x_i-\mu_k)^T\right] = 0</script>

<p>Similarly, we get the update equation for  $\Sigma_k$ at iteration $t+1$, and it depends on $\mu_k$. So again $\mu_k^{t+1}$ is calculated first and then applied to the update equation for $\Sigma_k$</p>

<script type="math/tex; mode=display">\Sigma_k^{t+1} = \frac{\sum_{i}\omega_{i,k}^t (x_i-\mu_k^{t+1})(x_i-\mu_k^{t+1})^T }{\sum_{i}\omega_{i,k}^t}</script>

<h2 id="5summary">5.Summary</h2>

<table border="1" style="width: 90%;margin:auto">
    <tr>
        <th style="text-align:center"></th>
        <th style="text-align:center">Univariate GMM</th>
        <th style="text-align:center">Multivariate GMM</th>
    </tr>
    <tr> 
	<th style="text-align:center">Init</th>
        <td>$$\alpha_{k}^0, \mu_k^0, \sigma_k^0$$</td>
        <td>$$\alpha_{k}^0, \mu_k^0, \Sigma_k^0$$</td>
    </tr>
    <tr>
        <th style="text-align:center">E-Step</th>
        <td>$$\omega_{i,k}^t = \frac{\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \sigma_k^t)}{\sum_{k}\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \sigma_k^t)}$$ </td>
        <td>$$\omega_{i,k}^t = \frac{\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \Sigma_k^t)}{\sum_{k}\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \Sigma_k^t)}$$ </td>
    </tr>
    <tr>
        <th style="text-align:center">M-Step</th>
        <td>$$
		\begin{aligned} 
		\alpha_k^{t+1} &amp;= \frac{\sum_{i}\omega_{i,k}^t}{N}\\ 
		\mu_k^{t+1} &amp;= \frac{\sum_{i}\omega_{i,k}^t x_i}{\sum_{i}\omega_{i,k}^t}\\
		(\sigma_k^2)^{t+1} &amp;= \frac{\sum_{i}\omega_{i,k}^t(x_i-\mu_k^{t+1})^2 }{\sum_{i}\omega_{i,k}^t}
		\end{aligned} 
		$$</td>
        <td>$$
		\begin{aligned} 
		\alpha_k^{t+1} &amp;= \frac{\sum_{i}\omega_{i,k}^t}{N}\\ 
		\mu_k^{t+1} &amp;= \frac{\sum_{i}\omega_{i,k}^tx_i}{\sum_{i}\omega_{i,k}^t}\\
		\Sigma_k^{t+1} &amp;= \frac{\sum_{i}\omega_{i,k}^t (x_i-\mu_k^{t+1})(x_i-\mu_k^{t+1})^T }{\sum_{i}\omega_{i,k}^t} 
		\end{aligned} 
		$$</td>
    </tr>
</table>

</div>
  <div class="share-page">
  <span style="float: left;">Share this on &rarr;&nbsp;&nbsp;</span>

  <!-- Twitter -->
  <a href="https://twitter.com/share" class="twitter-share-button" data-via="">Tweet</a>
  <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>

  <!-- Facebook -->
  <div class="fb-share-button" data-href="https://chenwj1989.github.io/post/en/gmm-em-en.html" data-layout="button_count" style="position: relative; top: -8px; left: 3px;"></div>
</div>

<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.6&appId=";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>

</div>


  
    
      
        
          
      
    
  
    
      
        
          
      
    
  
    
      
        
          
      
    
  
    
      
        
          
      
    
  
    
      
        
          
      
    
  
    
      
        
          
      
    
  
    
      
        
          
      
    
  
  

<div class="PageNavigation">
  
    <a class="prev" href="/post/cn/monaural-speech-enhancement-statistical-cn.html">&laquo; 单通道语音增强之统计信号模型</a>
  
  
    <a class="next" href="/post/cn/gmm-em-cn.html">高斯混合模型与EM算法的推导 &raquo;</a>
  
</div>

<div class="disqus-comments">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    /* <![CDATA[ */
    var disqus_shortname = "wjchen";
    var disqus_identifier = "https://chenwj1989.github.io_Gaussian Mixture Model and  Expectation-Maximization Algorithm";
    var disqus_title = "Gaussian Mixture Model and  Expectation-Maximization Algorithm";

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    /* ]]> */
  </script>
</div>

        <footer>
          &copy; wjchen
          
            - <a href="https://github.com/chenwj1989">https://github.com/chenwj1989</a> - Powered by Jekyll.
          
        </footer>
      </div>
      <!-- end /.col-sm-8 -->
    </div>
    <!-- end /.container -->

    <!-- Bootstrap core JavaScript -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="/static/js/jquery-1.11.0.min.js"></script>
    <script src="/static/js/jquery-migrate-1.2.1.min.js"></script>
    <script src="/static/js/bootstrap.min.js"></script>
    <script src="/static/js/super-search.js"></script>
    <script src="/static/js/thickbox-compressed.js"></script>
    <script src="/static/js/projects.js"></script>
    
    <script src="/static/katex/katex.min.js"></script>
    <script src="/static/katex/contrib/auto-render.min.js"></script>
    <script src="/static/katex/contrib/mathtex-script-type.min.js"></script>
    <script>
      renderMathInElement(
          document.body,
          {
              delimiters: [
                  {left: "$$", right: "$$", display: true},
                  {left: "\\[", right: "\\]", display: true},
                  {left: "$", right: "$", display: false},
                  {left: "\\(", right: "\\)", display: false}
              ]
          }
      );
    </script>

  </body>
</html>

