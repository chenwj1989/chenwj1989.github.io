<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>All Articles</title>
    <description>My Personal Thoughts</description>
    <link>https://chenwj1989.github.io/</link>
    <atom:link href="https://chenwj1989.github.io/sitemap.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 02 Mar 2020 21:32:47 +0800</pubDate>
    <lastBuildDate>Mon, 02 Mar 2020 21:32:47 +0800</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>频域LMS自适应滤波</title>
        <description>&lt;p&gt;本文代码位于&lt;a href=&quot;https://github.com/chenwj1989/MLSP/tree/master/adaptive_filtering&quot;&gt;GitHub&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#1-概述&quot; id=&quot;markdown-toc-1-概述&quot;&gt;1. 概述&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-lms和nlms算法&quot; id=&quot;markdown-toc-2-lms和nlms算法&quot;&gt;2. LMS和NLMS算法&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3-block-lms算法&quot; id=&quot;markdown-toc-3-block-lms算法&quot;&gt;3. Block LMS算法&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4-快速卷积&quot; id=&quot;markdown-toc-4-快速卷积&quot;&gt;4. 快速卷积&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#5-lms算法的快速计算频域自适应滤波&quot; id=&quot;markdown-toc-5-lms算法的快速计算频域自适应滤波&quot;&gt;5. LMS算法的快速计算：频域自适应滤波&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#6-减少时延滤波器分割的频域自适应滤波&quot; id=&quot;markdown-toc-6-减少时延滤波器分割的频域自适应滤波&quot;&gt;6. 减少时延：滤波器分割的频域自适应滤波&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1-概述&quot;&gt;1. 概述&lt;/h2&gt;

&lt;p&gt;下图是一个典型的自适应滤波场景， 输入信号$x(n)$经过一个位置的系统变换$h(z)$后得到参考信号$d(n)$。 自适应滤波器的目标是找出一组滤波器系数$w(z)$来逼近系统$h(z)$， 使输入信号经过$w(z)$变换后，与参考信号误差最小。&lt;/p&gt;

&lt;figure align=&quot;center&quot; style=&quot;width: 40%;margin:auto&quot;&gt;
  &lt;img width=&quot;auto&quot; height=&quot;auto&quot; src=&quot;/static/posts/2020/02/adaptive_filter.png&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;若使用FIR滤波器设计$w(z)$, 自适应滤波器的输出就是输入信号与滤波器权值的卷积：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y(n)=\sum_{i=0}^{N-1}w_ix(n-i)&lt;/script&gt;

&lt;p&gt;自适应滤波器的算法就是以误差$e(n)$为最小化目标，迭代求出最优的滤波器系数$w(z)$。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;e(n)=d(n)-y(n)&lt;/script&gt;

&lt;h2 id=&quot;2-lms和nlms算法&quot;&gt;2. LMS和NLMS算法&lt;/h2&gt;

&lt;p&gt;LMS是最广泛应用的自适应滤波算法，以MSE误差为目标函数，以梯度下降为优化算法。并且通常情况下，LMS以最新的输入计算的瞬时梯度替代实际梯度计算，类似于机器学习的随机梯度下降法。&lt;/p&gt;

&lt;p&gt;NLMS是使用输入的功率对步长进行归一化的方法，可以取得更好的收敛性能。&lt;/p&gt;

&lt;p&gt;时域上实现LMS和NLMS算法的参考资料很多，这里不赘述，下面列出算法迭代步骤。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;输入:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;　输入向量最新样本$\bm{x}(n)$&lt;/p&gt;

&lt;p&gt;　期望输出最新样本$\bm{d}(n)$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;输出:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;　滤波器系数$\bm{w}$,长度为M的FIR滤波器&lt;/p&gt;

&lt;p&gt;　滤波器输出$\bm{y}(n)$&lt;/p&gt;

&lt;p&gt;　滤波器输出与期望间的误差$e$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;初始化:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;　滤波器系数$\bm{w}(0)=zeros(1, M)$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;迭代过程:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;　for n = 0, 1, 2…&lt;/p&gt;

&lt;p&gt;　1.读取输入样本$\bm{x}(n)$ 和期望输出样本$\bm{d}(n)$&lt;/p&gt;

&lt;p&gt;　2.滤波：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bm{y}(n)=\bm{w}^T(n)\bm{x}(n)&lt;/script&gt;

&lt;p&gt;　3.计算误差：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;e(n)=\bm{d}(n) - \bm{y}(n)&lt;/script&gt;

&lt;p&gt;　4.更新系数：&lt;/p&gt;

&lt;p&gt;　　LMS:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bm{w}(n+1)=\bm{w}(n) + 2\mu e(n)\bm{x}(n)&lt;/script&gt;

&lt;p&gt;　　NLMS:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bm{w}(n+1)=\bm{w}(n) + \frac{\mu_0}{\bm{x}^T(n)\bm{x}(n)+\phi} e(n)\bm{x}(n)&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;算法复杂度:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;　LMS:  $2M+1$次乘法和$2M$次加法&lt;/p&gt;

&lt;p&gt;　NLMS: $3M+1$次乘法和$3M$次加法&lt;/p&gt;

&lt;h2 id=&quot;3-block-lms算法&quot;&gt;3. Block LMS算法&lt;/h2&gt;

&lt;p&gt;LMS算法对输入数据是串行处理的，每输入一个样本，都需要进行$2M+1$次乘法和$2M$次加法，对于长度为$N$的信号，计算时间复杂度为$O(NM)$。可以通过将输入数据分段并行处理，并且利用频域FFT来做快速卷积，大大减少计算复杂度。&lt;/p&gt;

&lt;p&gt;首先需要将串行的LMS算法转变为分块处理，也就是Block LMS(BLMS)。 每次迭代，输入数据被分成长度为$L$的块进行处理。和LMS使用瞬时梯度来进行滤波器参数更新不同，BLMS使用L点的平均梯度来进行参数更新。 也就是机器学习里面Stochastic Gradient Descent 和 Mini-Batch Gradient Descent的区别。 对第$k$块数据，BLMS算法递推公式为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bm{w}(k+1)=\bm{w}(k) + 2\mu_B \frac{\sum_{i=0}^{L-1}e(kL+i)\bm{x}(kL+i)}{L}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;输入:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;　输入向量$\bm{x}$&lt;/p&gt;

&lt;p&gt;　期望输出$\bm{d}$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;输出:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;　滤波器系数$\bm{w}$,长度为M的FIR滤波器&lt;/p&gt;

&lt;p&gt;　滤波器输出$\bm{y}$&lt;/p&gt;

&lt;p&gt;　滤波器输出与期望间的误差$\bm{e}$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;初始化:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;　滤波器系数$\bm{w}(0)=zeros(1, M)$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;迭代过程:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;　for k = 0, 1, 2 … N/L，读入第k块数据$\bm{x}$, $\bm{d}$&lt;/p&gt;

&lt;p&gt;　1. $\phi = zeros(1,L)$&lt;/p&gt;

&lt;p&gt;　2. for i = 0, 1, 2 … L-1&lt;/p&gt;

&lt;p&gt;　　2.1 滤波：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bm{y}(kL+i)=\bm{w}^T(k)\bm{x}(kL+i)&lt;/script&gt;

&lt;p&gt;　　2.2 计算误差：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bm{e}(kL+i)=\bm{d}(kL+i) - \bm{y}(kL+i)&lt;/script&gt;

&lt;p&gt;　　2.3 累计梯度：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi=\phi+ \mu \bm{e}(kL+i)\bm{x}(kL+i)&lt;/script&gt;

&lt;p&gt;　3 更新系数：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bm{w}(k+1)=\bm{w}(k) + \phi&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;算法复杂度:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;　$2M+1$次乘法和$2M+M/L$次加法&lt;/p&gt;

&lt;h2 id=&quot;4-快速卷积&quot;&gt;4. 快速卷积&lt;/h2&gt;

&lt;p&gt;前面一节描述的BLMS算法跟LMS算法相比，除了用块平均梯度代替瞬时梯度外，并没有不同。 为了提升卷积计算的复杂度，我们需要引入快速卷积。 也就是用FFT计算循环卷积，实现线性卷积的快速分块计算。&lt;/p&gt;

&lt;p&gt;长度为L的$x$和长度为M的$w$，线性卷积的结果是长度为$L+M-1$的$y$。 时域卷积，频域是相乘，因此有&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y(n)=x(n)*w(n)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{FT}\left[y(n)\right]= \mathcal{FT}\left[x(n)\right] \mathcal{FT}\left[w(n)\right]&lt;/script&gt;

&lt;p&gt;傅立叶变换在频域上是连续的，计算机做傅立叶运算会将频域离散化采样，转化成$N$点离散傅立叶变换(DFT)，并且用FFT算法做快速计算。 将卷积的输入$x$和$w$都补零成$N$点，做一个$N$点的DFT，其逆变换就是循环卷积的结果，时域上是周期为$N$的重复信号。&lt;/p&gt;

&lt;figure align=&quot;center&quot; style=&quot;width: 70%;margin:auto&quot;&gt;
  &lt;img width=&quot;auto&quot; height=&quot;auto&quot; src=&quot;/static/posts/2020/02/linear_conv_with_circular_conv.png&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;如上图所示，只有取DFT点数$N&amp;gt;L+M-1$，才能防止卷积结果在时域上面混叠。IDFT结果的前$M+L-1$个值就是所需的卷积结果。&lt;/p&gt;

&lt;p&gt;于是两组有限长信号的卷积，则转换成3次DFT计算。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y(n)= \mathcal{IDFT}\left\{\mathcal{DFT}\left[x(n)\right] \mathcal{DFT}\left[w(n)\right]\right\}&lt;/script&gt;

&lt;p&gt;直接计算卷积， 大约需要$N^2$次乘法和$N^2$次加法。&lt;/p&gt;

&lt;p&gt;采用FFT算法，3次DFT计算需要$N+\frac{3N}{2}\log_2N$次复数乘法和$3N\log_2N$次复数加法，相对直接计算算法复杂度从$O(N^2)$降到了$O(N\log_2N)$。&lt;/p&gt;

&lt;p&gt;前面说的是两段有限长信号作一次卷积的流程。回到BLMS，对信号$x$进行分段处理。每段输入长度是$B$， 滤波器长度是$M$。&lt;/p&gt;

&lt;p&gt;如果在$w$后面补$K-B$个零，而在$x$当前块前部保留前一块的最后$K-B$个点，做$K$点快速卷积，结果中最后$B$点是有效输出，而前$K-B$点可丢弃。这种分块处理长卷积的方法，叫Overlap-and-Save方法。用FFT快速卷积+Overlap-and-Save方法，就可以高效处理BLMS滤波。&lt;/p&gt;

&lt;figure align=&quot;center&quot; style=&quot;width: 100%;margin:auto&quot;&gt;
  &lt;img width=&quot;auto&quot; height=&quot;auto&quot; src=&quot;/static/posts/2020/02/fast_conv_ols.png&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;5-lms算法的快速计算频域自适应滤波&quot;&gt;5. LMS算法的快速计算：频域自适应滤波&lt;/h2&gt;

&lt;p&gt;使用快速卷积的方法实现BLMS，就是频域实现的LMS自适应滤波器。称作Fast Block LMS (FBLMS) 或者 Frequency Domain Adaptive Filter (FDAF)。&lt;/p&gt;

&lt;p&gt;对于长度为$M$的滤波器，FBLMS一般采用$2M$点FFT，且使用Overlap-and-Savee的快速卷积方法。也就是说，滤波器向量$w$做FFT前须补$M$个零值。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;W=FFT\left[ \begin{matrix} w\\ 0 \end{matrix} \right]&lt;/script&gt;

&lt;p&gt;输入向量的分块长度也设为$M$，则FFT的输入前$M$点是保存前一块的数据，后$M$点是当前块数据。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X=FFT\left[ \begin{matrix} x_{k-1}\\ x_k \end{matrix} \right]&lt;/script&gt;

&lt;p&gt;使用overlap-save快速卷积方法实现BLMS中的卷积部分，当前块输出向量$y$为IFFT的后$M$点。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left[ \begin{matrix} C\\ y_k \end{matrix} \right]=IFFT\left[ 
FFT\left[ \begin{matrix} w\\ 0 \end{matrix} \right] 
FFT\left[ \begin{matrix} x_{k-1}\\ x_k \end{matrix} \right]\right]&lt;/script&gt;

&lt;p&gt;梯度的计算，可以将误差与输入信号放到频域来做，具体推导参考&lt;a href=&quot;http://www.cs.tut.fi/~tabus/course/ASP/SGN2206LectureNew6.pdf&quot;&gt;Block Adaptive Filters and Frequency Domain Adaptive Filters&lt;/a&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left[ \begin{matrix} \phi\\ D \end{matrix} \right]=IFFT\left[ 
FFT\left[ \begin{matrix} 0\\ e \end{matrix} \right] 
FFT\left[ \begin{matrix} x_{k-1}\\ x_k \end{matrix} \right]'\right]&lt;/script&gt;

&lt;figure align=&quot;center&quot; style=&quot;width: 100%;margin:auto&quot;&gt;
  &lt;img width=&quot;auto&quot; height=&quot;auto&quot; src=&quot;/static/posts/2020/02/fblms.png&quot; /&gt;
  &lt;figcaption&gt;文献[1]中提供的FBLMS框图&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;输入:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;　分块输入信号$\bm{x_k}$, 块长度为M&lt;/p&gt;

&lt;p&gt;　分块参考信号$\bm{d_k}$, 块长度为M&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;输出:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;　更新的滤波器系数$\bm{w_k}$,长度为M的FIR滤波器&lt;/p&gt;

&lt;p&gt;　分块输出信号$\bm{y_k}$, 块长度为M&lt;/p&gt;

&lt;p&gt;　滤波器输出与参考信号间的误差$\bm{e}$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;初始化:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;　滤波器系数$\bm{w}_0=zeros(M,1)$&lt;/p&gt;

&lt;p&gt;　初始数据块$\bm{x}_0=zeros(M,1)$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;迭代过程:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;　for $k = 1, 2 … N/L$，读入第$k$块数据$\bm{w_k}$, $\bm{d_k}$&lt;/p&gt;

&lt;p&gt;　1. $\bm{\phi} = zeros(M,1)$&lt;/p&gt;

&lt;p&gt;　2. 计算块输出&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left[ \begin{matrix} C\\ y_k \end{matrix} \right]=IFFT\left[ 
FFT\left[ \begin{matrix} w_k\\ 0 \end{matrix} \right] 
FFT\left[ \begin{matrix} x_{k-1}\\ x_k \end{matrix} \right]\right]&lt;/script&gt;

&lt;p&gt;　3. 计算误差： $\bm{e} = \bm{y}_k - \bm{d}_k$&lt;/p&gt;

&lt;p&gt;　4. 计算梯度：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left[ \begin{matrix} \phi\\ D \end{matrix} \right]=IFFT\left[ 
FFT\left[ \begin{matrix} 0\\ e \end{matrix} \right] 
\overline{FFT\left[ \begin{matrix} x_{k-1}\\ x_k \end{matrix} \right]}\right]&lt;/script&gt;

&lt;p&gt;　5. 更新滤波器：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;FFT\left[ \begin{matrix} w_{k+1}\\ 0 \end{matrix} \right]= 
FFT\left[ \begin{matrix} w_k\\ 0 \end{matrix} \right] +
\mu FFT\left[ \begin{matrix} \phi\\ 0 \end{matrix} \right]&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;算法复杂度:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;　$10M\log M+26M$&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;process&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; 
	&lt;span class=&quot;s&quot;&gt;'''
	Process a block data, and updates the adaptive filter (optional)
	
	Parameters
	----------
	x_b: float
	the new input block signal
	d_b: float
	the new reference block signal
	update: bool, optional
	whether or not to update the filter coefficients
	'''&lt;/span&gt;  
	&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concatenate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	
	&lt;span class=&quot;c&quot;&gt;# block-update parameters&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y_2B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ifft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;real&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_2B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt;
	
	&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
	
	&lt;span class=&quot;c&quot;&gt;# Update the parameters of the filter&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;E&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concatenate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# (2B)&lt;/span&gt;
	
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nlms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-10&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;E&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;E&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# Set the upper bound of E, to prevent divergence&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;m_errThreshold&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;Enorm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;E&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# (2B)&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# print(E)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eidx&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Enorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eidx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m_errThreshold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;E&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eidx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_errThreshold&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;E&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eidx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Enorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eidx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
	
	&lt;span class=&quot;c&quot;&gt;# Compute the correlation vector (gradient constraint)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;phi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;einsum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'i,i-&amp;gt;i'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;E&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# (2B)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;phi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ifft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;phi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;phi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;phi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;phi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	
	&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;phi&lt;/span&gt;
	&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;real&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ifft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[:&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; 
	
&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;6-减少时延滤波器分割的频域自适应滤波&quot;&gt;6. 减少时延：滤波器分割的频域自适应滤波&lt;/h2&gt;

&lt;p&gt;前面是将输入信号分块处理，提高算法效率。当FIR滤波器抽头数量很大时，FBLMS每M点计算一次输出和更新滤波器，造成比较大的延时。&lt;/p&gt;

&lt;p&gt;一种想法是将滤波器也进行分割，这种改进延时的滤波器有几种名字：
Partitioned Fast Block LMS (PFBLMS)，Frequency Domain Adaptive Filter (PBFDAF)， Multi-DelayBlock Filter(MDF)， 都是一回事。&lt;/p&gt;

&lt;figure align=&quot;center&quot; style=&quot;width: 70%;margin:auto&quot;&gt;
  &lt;img width=&quot;auto&quot; height=&quot;auto&quot; src=&quot;/static/posts/2020/02/conv.png&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;如果将长度为$M$的滤波器$w$等分为长度为$B$的小段，$M=P*B$。则卷积的结果可以分解为$P$个卷积之叠加。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y(n)=\sum_{l=0}^{P} y_l(n)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_l(n)=\sum_{i=0}^{B-1} w_{i+lB}x(n-lB-i)&lt;/script&gt;

&lt;figure align=&quot;center&quot; style=&quot;width: 70%;margin:auto&quot;&gt;
  &lt;img width=&quot;auto&quot; height=&quot;auto&quot; src=&quot;/static/posts/2020/02/partitioned_conv.png&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;于是一段线性卷积被分解成$P$个线性卷积，并且可以用FFT+OLS分别计算这$P$个卷积。 这样做好处是，每次迭代只需要输入长度为$B$的信号块，保留原buffer中的后$P-1$段，与最新的一段作为新的输入，就可以重复以上的$P$段卷积叠加。每次迭代，可以更新$B$点输出信号。&lt;/p&gt;

&lt;p&gt;PFBLMS算法的时延就只有FBLMS的$1/P$，极大地改善了滤波器的可用性。Speex和WebRTC的回声消除代码都使用了这种结构的滤波器。&lt;/p&gt;

&lt;figure align=&quot;center&quot; style=&quot;width: 100%;margin:auto&quot;&gt;
  &lt;img width=&quot;auto&quot; height=&quot;auto&quot; src=&quot;/static/posts/2020/02/pfblms.png&quot; /&gt;
  &lt;figcaption&gt;文献[1]中提供的PFBLMS框图，其中M是我文中的B&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;输入:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;　分块输入信号$\bm{x_k}$, 块长度为B&lt;/p&gt;

&lt;p&gt;　分块参考信号$\bm{d_k}$, 块长度为B&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;输出:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;　滤波器系数$\bm{w}$,长度为M=PB的FIR滤波器&lt;/p&gt;

&lt;p&gt;　分块输出信号$\bm{y_k}$, 块长度为B&lt;/p&gt;

&lt;p&gt;　滤波器输出与期望间的误差$\bm{e}$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;初始化:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;　滤波器系数$\bm{w}_0=zeros(B,P)$&lt;/p&gt;

&lt;p&gt;　初始数据块$\bm{x}_0=zeros(B,P)$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;迭代过程:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;　for k = 0, 1, 2 … N/B，读入第k块数据$x_k$, $d_k$&lt;/p&gt;

&lt;p&gt;　1. $\bm{\phi} = zeros(B,1)$&lt;/p&gt;

&lt;p&gt;　2. 滑动重组输入信号，并且计算FFT。&lt;/p&gt;

&lt;p&gt;　（实际只需计算最后一列，前P-1列可以使用上一次迭代保留结果）&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
Xf_k =
FFT\left[ \begin{matrix} x_{k-P}&amp;...&amp; x_{k-2} &amp; x_{k-1}\\x_{k-P+1}&amp;...&amp; x_{k-1} &amp; x_k \end{matrix} \right] %]]&gt;&lt;/script&gt;

&lt;p&gt;　2. 计算块输出&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left[ \begin{matrix} C\\ y_k \end{matrix} \right]=IFFT\left[ 
FFT\left[ \begin{matrix} w_k\\ 0 \end{matrix} \right] Xf_k\right]&lt;/script&gt;

&lt;p&gt;　3. 计算误差： $e = y_k - d_k$&lt;/p&gt;

&lt;p&gt;　4. 计算梯度：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left[ \begin{matrix} \phi\\ D \end{matrix} \right]=IFFT\left[ 
FFT\left[ \begin{matrix} 0\\ e \end{matrix} \right] 
\overline{Xf_k}\right]&lt;/script&gt;

&lt;p&gt;　5. 更新滤波器：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;FFT\left[ \begin{matrix} w_{k+1}\\ 0 \end{matrix} \right]= 
FFT\left[ \begin{matrix} w_k\\ 0 \end{matrix} \right] +
\mu FFT\left[ \begin{matrix} \phi\\ 0 \end{matrix} \right]&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;算法复杂度:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;　与FBLMS相仿&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;process&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; 
    &lt;span class=&quot;s&quot;&gt;'''
	Process a block data, and updates the adaptive filter (optional)
	
	Parameters
	----------
	x_b: float
	the new input block signal
	d_b: float
	the new reference block signal
	update: bool, optional
	whether or not to update the filter coefficients
	'''&lt;/span&gt;
	&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concatenate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;Xf_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	
	&lt;span class=&quot;c&quot;&gt;# block-update parameters&lt;/span&gt;
	&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Xf : Mx2B  sliding window&lt;/span&gt;
	&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xf_b&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Xf : Mx2B  sliding window&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y_2B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ifft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Wf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [Px2B] element multiply [Px2B] , then ifft&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;real&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_2B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt;
	
	&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
	
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;E&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concatenate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# (2B)&lt;/span&gt;
	
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nlms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xf_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-6&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;E&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;E&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;
	
	&lt;span class=&quot;c&quot;&gt;# Set the upper bound of E, to prevent divergence&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;m_errThreshold&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;Enorm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;E&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# (2B)&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# print(E)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eidx&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Enorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eidx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m_errThreshold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;E&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eidx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_errThreshold&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;E&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eidx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Enorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eidx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    
	
	
	&lt;span class=&quot;c&quot;&gt;# Update the parameters of the filter&lt;/span&gt;
	&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Wf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Wf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;E&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	
	&lt;span class=&quot;c&quot;&gt;# Compute the correlation vector (gradient constraint)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;waux&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ifft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Wf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;waux&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
	&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Wf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;waux&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	
	&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;real&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ifft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Wf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参考文献&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[1] B. Farhang-Boroujeny, Adaptive Filters: theory and applications. John Wiley &amp;amp; Sons, 2013.&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;http://www.cs.tut.fi/~tabus/course/ASP/SGN2206LectureNew6.pdf&quot;&gt;Block Adaptive Filters and Frequency Domain Adaptive Filters&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Sat, 29 Feb 2020 01:00:00 +0800</pubDate>
        <link>https://chenwj1989.github.io/post/cn/freq-domain-lms-cn.html</link>
        <guid isPermaLink="true">https://chenwj1989.github.io/post/cn/freq-domain-lms-cn.html</guid>
        
        
        <category>机器学习</category>
        
        <category>信号处理</category>
        
      </item>
    
      <item>
        <title>单通道语音增强之WebRTC去噪算法</title>
        <description>&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#1-概述&quot; id=&quot;markdown-toc-1-概述&quot;&gt;1. 概述&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1-概述&quot;&gt;1. 概述&lt;/h2&gt;

&lt;p&gt;WebRTC NS 去噪模块流程图：&lt;/p&gt;

&lt;figure align=&quot;center&quot; style=&quot;width: 100%;margin:auto&quot;&gt;
  &lt;img width=&quot;auto&quot; height=&quot;auto&quot; src=&quot;/static/posts/2019/webrtc_ns.png&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Sun, 24 Nov 2019 01:00:00 +0800</pubDate>
        <link>https://chenwj1989.github.io/post/cn/monaural-speech-enhancement-webrtc-cn.html</link>
        <guid isPermaLink="true">https://chenwj1989.github.io/post/cn/monaural-speech-enhancement-webrtc-cn.html</guid>
        
        
        <category>语音</category>
        
      </item>
    
      <item>
        <title>单通道语音增强之综述</title>
        <description>&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#1-概述&quot; id=&quot;markdown-toc-1-概述&quot;&gt;1. 概述&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-核心模块&quot; id=&quot;markdown-toc-2-核心模块&quot;&gt;2. 核心模块&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#21--抑制增益和时频掩模&quot; id=&quot;markdown-toc-21--抑制增益和时频掩模&quot;&gt;2.1  抑制增益和时频掩模&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#211-谱减法&quot; id=&quot;markdown-toc-211-谱减法&quot;&gt;2.1.1 谱减法&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#212-维纳滤波&quot; id=&quot;markdown-toc-212-维纳滤波&quot;&gt;2.1.2 维纳滤波&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#213-统计信号方法&quot; id=&quot;markdown-toc-213-统计信号方法&quot;&gt;2.1.3 统计信号方法&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#214-理想二值掩模&quot; id=&quot;markdown-toc-214-理想二值掩模&quot;&gt;2.1.4 理想二值掩模&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#22-话音激活检测和语音存在概率&quot; id=&quot;markdown-toc-22-话音激活检测和语音存在概率&quot;&gt;2.2 话音激活检测和语音存在概率&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#23-噪声估计&quot; id=&quot;markdown-toc-23-噪声估计&quot;&gt;2.3 噪声估计&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#231-最小值跟踪算法&quot; id=&quot;markdown-toc-231-最小值跟踪算法&quot;&gt;2.3.1 最小值跟踪算法&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#232-直方图噪声估计算法&quot; id=&quot;markdown-toc-232-直方图噪声估计算法&quot;&gt;2.3.2 直方图噪声估计算法&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#233-递归平均噪声算法&quot; id=&quot;markdown-toc-233-递归平均噪声算法&quot;&gt;2.3.3 递归平均噪声算法&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#234-基于语音概率的递归平均算法&quot; id=&quot;markdown-toc-234-基于语音概率的递归平均算法&quot;&gt;2.3.4 基于语音概率的递归平均算法&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#235-最小值控制的递归平均mcra算法&quot; id=&quot;markdown-toc-235-最小值控制的递归平均mcra算法&quot;&gt;2.3.5 最小值控制的递归平均（MCRA）算法&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#24-snr估计&quot; id=&quot;markdown-toc-24-snr估计&quot;&gt;2.4 SNR估计&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1-概述&quot;&gt;1. 概述&lt;/h2&gt;

&lt;p&gt;单通道语音增强是语音信号处理中广泛研究的课题，主要作为前端去噪模块应用在提升音质、语音通信、辅助听觉、语音识别等领域。 单通道语音增强问题定义主要包括两个方面：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;输入信号是只有一路通道的带噪语音信号。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;处理目标是增强语音，降低噪声，也相当于分离“语音”与“非语音”信号。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;不包括：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;多通道语音信号处理， 那属于阵列信号处理的范畴。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;区分/分离混合的不同人的语音（也就是鸡尾酒会问题），那属于盲源分离的范畴。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;单通道语音增强传统的方法是滤波和统计信号处理，比如WebRTC的噪声抑制模块就是用维纳滤波。 这些传统的方法基本都在 《语音增强–理论与实践》一书中有详细讲解。&lt;/p&gt;

&lt;p&gt;近几年机器学习方法兴起，也逐渐成为语音增强的主要研究方向，各种新型神经网络的方法都被尝试用在语音增强领域。这些新方法主要看近几年的InterSpeech会议、ICASSP会议和IEEE的期刊。&lt;/p&gt;

&lt;p&gt;下面先对单通道语音增强号的基本处理步骤做个简单介绍。&lt;/p&gt;

&lt;p&gt;假设麦克风采集到的带噪语音序列为$y[n]$，并且噪声都是加性噪声。则带噪语音序列为无噪语音序列与噪声序列的和。 原始语音信号与噪声均可视为随机信号。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y[n] = x[n] + d[n]&lt;/script&gt;

&lt;p&gt;语音信号的处理一般都在频域，需要对带噪信号$y[n]$进行分帧、加窗、短时傅里叶变换（STFT）后，得到每一帧的频域信号，其中X，Y，D分别是干净语音、带噪信号和噪声的频域信号。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Y(\omega_{k}) = X(\omega_{k}) + D(\omega_{k})&lt;/script&gt;

&lt;p&gt;语音增强的目标是对实际信号$X(\omega_{k})$的幅度和相位进行估计。但是因为相位不易估计、而且研究表明相位对去噪效果影响比较小\cite{wang1982unimportance}，所以大部分方法都只对幅度谱进行增强，而相位则沿用带噪信号的相位。&lt;/p&gt;

&lt;p&gt;换句话说，语音增强就是要找出一个频域的实函数$H_{\omega_{k}}$, 并且将这个函数与带噪信号相乘，得到干净语音的估计。这个实函数称作抑制增益(Suppression Gain)。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{X}(\omega_{k}) = H(\omega_{k})|Y(\omega_{k})|e^{j\theta_{y}(k)}&lt;/script&gt;

&lt;p&gt;下面是单通道语音增强系统主要步骤的示意图，系统目标就是估计抑制增益，而抑制增益依赖于两个核心步骤：语音检测VAD和噪声估计模块。只有准确估计噪声谱$D(\omega_{k})$，才有可能准确估计抑制增益。 详细的VAD和噪声估计方法不在这篇文章里面详述，具体可以看参考文献。 一种简单的想法是先估计出VAD，如过判断此帧没有语音，则更新噪声谱，否则就沿用上一帧的噪声谱。&lt;/p&gt;

&lt;figure align=&quot;center&quot; style=&quot;width: 100%;margin:auto&quot;&gt;
  &lt;img width=&quot;auto&quot; height=&quot;auto&quot; src=&quot;/static/posts/2019/ns_detailed_flow.png&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;综上，语音增强的典型流程就是：
1 对带噪语音y[n]分帧， 每一帧进行DFT得到$Y(\omega_{k})$。
2 利用$Y(\omega_{k})$进行VAD检测和噪声估计。
3 计算抑制增益$H_{k}$。
4 抑制增益$H_{k}$与带噪信号谱相乘，得到纯净语音谱$\hat{X}(\omega_{k})$
5 对$\hat{X}(\omega_{k})$进行IDFT,得到纯净语音序列的估计$x[n]$。&lt;/p&gt;

&lt;p&gt;噪声估计模块可以估计噪声功率，也可以估计信噪比，避免信号幅度变化带来的误差。
定义后验信噪比：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\gamma_k = \frac{P_{yy}}{P_{dd}}&lt;/script&gt;

&lt;p&gt;定义先验信噪比：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\xi_k = \frac{P_{xx}}{P_{dd}}&lt;/script&gt;

&lt;h2 id=&quot;2-核心模块&quot;&gt;2. 核心模块&lt;/h2&gt;

&lt;h3 id=&quot;21--抑制增益和时频掩模&quot;&gt;2.1  抑制增益和时频掩模&lt;/h3&gt;

&lt;h4 id=&quot;211-谱减法&quot;&gt;2.1.1 谱减法&lt;/h4&gt;

&lt;p&gt;谱减法是最直观的去噪声思想，就是带噪信号减去噪声的频谱，就等于干净信号的频谱。估计信号频谱的表达式如下，其中$\hat{D}(\omega_{k})$应是噪声估计模块得到的噪声频谱。&lt;/p&gt;

&lt;p&gt;假设语音信号与噪声不相关，于是得到估计的信号功率谱是测量信号功率谱减去估计的噪声功率谱。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;|X(\omega_{k})|^2 =  |Y(\omega_{k})|^2 -  |D(\omega_{k})|^2
P_{xx}(\omega_{k}) =  P_{yy}(\omega_{k}) -  P_{dd}(\omega_{k})&lt;/script&gt;

&lt;p&gt;因此抑制增益函数即为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(\omega_{k}) =  \sqrt{\frac{P_{yy}(\omega_{k}) -  P_{dd}(\omega_{k})}{P_{yy}(\omega_{k})}} = \sqrt{\frac{\xi_k}{\xi_k+1}}&lt;/script&gt;

&lt;h4 id=&quot;212-维纳滤波&quot;&gt;2.1.2 维纳滤波&lt;/h4&gt;

&lt;p&gt;维纳滤波的思想也很直接，就是将带噪信号经过线性滤波器变换来逼近原信号，并求均方误差最小时的线性滤波器参数。维纳滤波语音增强的目标就是寻找系数为实数的线性滤波器，使得滤波偶信号与原干净语音信号之间的均方误差最小。这是一个优化问题，目标是求使得均方误差最小的参数$H$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(\omega_{k}) =\mathop{\arg\min}_{H(\omega_{k})} E\left[\left[X(\omega_{k}) - H(\omega_{k})Y(\omega_{k})\right]\left[X(\omega_{k}) - H(\omega_{k})Y(\omega_{k})\right]^*\right]&lt;/script&gt;

&lt;p&gt;Gain用先验信噪比表示&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(\omega_{k}) = \frac{P_{xx}}{P_{yy}} = \frac{P_{xx}}{P_{xx} +P_{dd} } = \frac{\xi_k}{\xi_k + 1 }&lt;/script&gt;

&lt;h4 id=&quot;213-统计信号方法&quot;&gt;2.1.3 统计信号方法&lt;/h4&gt;

&lt;p&gt;见博文&lt;a href=&quot;https://wjchen.net/post/cn/monaural-speech-enhancement-statistical-cn.html&quot;&gt;《单通道语音增强之统计信号模型》&lt;/a&gt;。&lt;/p&gt;

&lt;h4 id=&quot;214-理想二值掩模&quot;&gt;2.1.4 理想二值掩模&lt;/h4&gt;

&lt;p&gt;待补充。&lt;/p&gt;

&lt;h3 id=&quot;22-话音激活检测和语音存在概率&quot;&gt;2.2 话音激活检测和语音存在概率&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;话音激活检测(Voice Activity Detection, VAD)&lt;/strong&gt; 将语音帧二分为“纯噪声”和“语音噪声混合”两类。 说话人静音、停顿都会出现多帧的纯噪声，对这些帧无需估计语音信号，而可以用来估计噪声功率。 语音帧经过VAD分类后，进行不同的处理:&lt;/p&gt;

&lt;p&gt;$H_0$：不含语音帧，更新噪声功率估计和Gain, 进行抑制；&lt;/p&gt;

&lt;p&gt;$H_1$：包含语音帧，沿用上一帧的噪声功率和Gain，进行抑制。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;语音存在概率SPP(Speech Presence Probability，SPP)&lt;/strong&gt; 跟VAD作二分类不同，利用统计模型对每一帧估计出一个取值在[0,1]的语音存在概率，也就是一种soft-VAD。 SPP通常跟统计信号模型结合起来估计最终的Gain。&lt;/p&gt;

&lt;p&gt;一种估计SPP的方法是根据测量信号$Y(\omega_{k})$估计每个频点的语音存在的后验概率，也就是
&lt;script type=&quot;math/tex&quot;&gt;P(H_1^k|Y(\omega_{k}))&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;根据贝叶斯公式：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(H_1^k|Y(\omega_{k})) = \frac{P(Y(\omega_{k})|H_1^k)P(H_1)}{P(Y(\omega_{k})|H_1^k)P(H_1)+P(Y(\omega_{k})|H_0^k)P(H_0)}&lt;/script&gt;

&lt;p&gt;定义语音不存在的先验概率$P(H_0^k)$为$q_k$, 语音存在的先验概率$P(H_1^k)$为$1-q_k$。假设噪声与语音信号为零均值复高斯分布。最终可以得到SPP计算公式：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(H_1^k|Y(\omega_{k})) = \frac{1-q_k}{1-q_k+q_k(1+\xi_k')\exp(-\nu_k')}&lt;/script&gt;

&lt;p&gt;其中为$\xi_k’$为条件信噪比，有$\xi_k’=\frac{\xi_k}{1-q_k}$ 及 $\nu_k’=\frac{\xi_k’}{\xi_k’+1}\gamma_k$。&lt;/p&gt;

&lt;p&gt;语音不存在的先验概率$q_k$可以采用经验值，如0.5，或者进行累加平均， 也可以参考《语音增强—理论与实践》中更复杂的算法。&lt;/p&gt;

&lt;h3 id=&quot;23-噪声估计&quot;&gt;2.3 噪声估计&lt;/h3&gt;

&lt;h4 id=&quot;231-最小值跟踪算法&quot;&gt;2.3.1 最小值跟踪算法&lt;/h4&gt;

&lt;p&gt;最小值跟踪发的思想是，噪声能量比较平稳， 带语音的片段能量总是大于纯噪声段。 对于每个频点，跟踪一段时间内最低的功率，那就是纯噪声的功率。&lt;/p&gt;

&lt;p&gt;为了使功率估计更稳定，通常要对功率谱进行平滑处理：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\lambda, k) = \alpha P(\lambda-1,k) + (1-\alpha)|Y(\lambda, k)|^2&lt;/script&gt;

&lt;p&gt;然后寻找当前第$\lambda$帧的最低功率$P_{min}$。简单的方法是直接比较前$D$帧的功率，得到最小值，计算速度较慢。
还有一种方法是对$P_{min}$进行非线性平滑，公式如下。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\text{if    } &amp;P_{min}(\lambda-1, k) &lt; P(\lambda, k) \\
&amp;P_{min}=\gamma P_{min}(\lambda-1, k) + \frac{1-\gamma}{1-\beta}\left[P(\lambda, k)-\beta P(\lambda-1, k)\right]\\
\text{else}&amp;\\
&amp;P_{min}=\gamma P_{min}(\lambda-1, k)\\
\text{end}&amp;
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;参数需要调优，可以参考文献中提供的取值：$\alpha=0.7$、$\beta=0.96$、$\gamma=0.998$。&lt;/p&gt;

&lt;h4 id=&quot;232-直方图噪声估计算法&quot;&gt;2.3.2 直方图噪声估计算法&lt;/h4&gt;
&lt;p&gt;这种估计方法的思路是，噪声的能量变化比语音稳定，因此按频点统计一段时间内的能量直方图，每个频点出现频次最高的能量值就是噪声的能量。 主要包括以下几个步骤：&lt;/p&gt;

&lt;p&gt;1.计算当前帧的功率谱
&lt;script type=&quot;math/tex&quot;&gt;|Y(\lambda, k)^2|&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;2.计算当前帧前连续D帧功率谱密度直方图，选择每个频点k的频次最高的功率值$H_{max}(\lambda, k)$&lt;/p&gt;

&lt;p&gt;3.滑动平均，更新噪声功率谱密度
	&lt;script type=&quot;math/tex&quot;&gt;\hat{\sigma}_d^2(\lambda, k) = \alpha_m\hat{\sigma}_d^2(\lambda-1, k) + (1-\alpha_m)H_{max}(\lambda, k)&lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;233-递归平均噪声算法&quot;&gt;2.3.3 递归平均噪声算法&lt;/h4&gt;
&lt;p&gt;当前帧的SNR很低，或者语音出现概率很低时，意味着当前信号功率很接近噪声功率，我们可以用当前帧的功率谱与前一帧估计的噪声功率进行加权平均，从而更新噪声功率谱。这就是递归平均法，通用的公式是：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\sigma}_d^2(\lambda, k) = \alpha(\lambda, k)\hat{\sigma}_d^2(\lambda-1, k) + \left[1-\alpha(\lambda,k)\right]|Y(\lambda, k)|^2&lt;/script&gt;

&lt;p&gt;算法的核心变成了计算参数$\alpha(\lambda, k)$，研究者提出了不同的方法，比如可以根据后验信噪比$\gamma_k(\lambda)$计算参数：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha(\lambda, k)=1-\text{min}\left\{1, \frac{1}{\gamma_k^p(\lambda)}\right\}&lt;/script&gt;

&lt;h4 id=&quot;234-基于语音概率的递归平均算法&quot;&gt;2.3.4 基于语音概率的递归平均算法&lt;/h4&gt;
&lt;p&gt;用$H_1^k$和$H_0^k$分别代表当前帧包含语音和不包含语音，从概率论的角度，当前帧的噪声功率期望值为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\hat{\sigma}_d^2(\lambda, k) &amp;= E\left[\sigma_d^2(\lambda, k)|Y(\lambda, k)\right] \\
&amp;= E\left[\sigma_d^2(\lambda, k)|H_0^k\right]P(H_0^k|Y(\lambda, k)) + E\left[\sigma_d^2(\lambda, k)|H_1^k\right]P(H_1^k|Y(\lambda, k))
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;其中，当前帧不存在语音时，噪声功率就是信号功率，所以
&lt;script type=&quot;math/tex&quot;&gt;E\left[\sigma_d^2(\lambda, k)|H_0^k\right]= |Y(\lambda, k)|^2&lt;/script&gt;。当前帧存在语音时，可以用前一帧估计的噪声功率来近似,
&lt;script type=&quot;math/tex&quot;&gt;E\left[\sigma_d^2(\lambda, k)|H_1^k\right]=\hat{\sigma}_d^2(\lambda-1, k)&lt;/script&gt;。噪声的递归平均算法转化为求当前帧每个频点的语音存在/不存在概率问题：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\sigma}_d^2(\lambda, k) = |Y(\lambda, k)|^2P(H_0^k|Y(\lambda, k)) + \hat{\sigma}_d^2(\lambda-1, k)P(H_1^k|Y(\lambda, k))&lt;/script&gt;

&lt;p&gt;比照递归平均的通用公式，也就是
&lt;script type=&quot;math/tex&quot;&gt;\alpha(\lambda, k) =P(H_1^k|Y(\lambda, k))&lt;/script&gt;。 使用前一节介绍的语音存在概率SPP计算方法求
&lt;script type=&quot;math/tex&quot;&gt;P(H_1^k|Y(\lambda, k))&lt;/script&gt;即可。&lt;/p&gt;

&lt;h4 id=&quot;235-最小值控制的递归平均mcra算法&quot;&gt;2.3.5 最小值控制的递归平均（MCRA）算法&lt;/h4&gt;
&lt;p&gt;MCRA是一种将最小值跟踪与基于语音概率的递归平均结合起来的算法,核心思想是用当前帧功率谱平滑后与局部最小功率谱密度之比来估计语音概率。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S_r(\lambda, k) = \frac{S(\lambda, k)}{S_{min}(\lambda, k)}&lt;/script&gt;

&lt;p&gt;以某阈值$\delta$对语音概率$p(\lambda, k)$进行二元估计&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\lambda, k) = \left(S_r(\lambda, k)&gt;\delta\right)?1:0&lt;/script&gt;

&lt;p&gt;语音概率也可以进行平滑：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(\lambda, k) = \alpha_p\hat{p}(\lambda-1, k) + (1-\alpha_p)p(\lambda, k)&lt;/script&gt;

&lt;p&gt;另外，如果将语音不存在是的噪声估计也做滑动平均，也就是&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\hat{\sigma}_d^2(\lambda, k) = \alpha\hat{\sigma}_d^2(\lambda-1, k)+(1-\alpha)|Y(\lambda, k)|^2&lt;/script&gt;
可以得到最终的噪声概率估计公式：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\sigma}_d^2(\lambda, k) = \alpha_d(\lambda, k)\hat{\sigma}_d^2(\lambda-1, k) + \left[1-\alpha_d(\lambda,k)\right]|Y(\lambda, k)|^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha_d(\lambda, k) = \alpha + (1-\alpha)\hat{p}(\lambda, k)&lt;/script&gt;

&lt;h3 id=&quot;24-snr估计&quot;&gt;2.4 SNR估计&lt;/h3&gt;

&lt;p&gt;后验信噪比的估计比较直接，就是带噪信号功率与估计噪声功率之比： $\sigma_k^2$。然后$\gamma_k = Y_k^2/\sigma_k^2$。&lt;/p&gt;

&lt;p&gt;先验信噪比是纯净信号功率与噪声功率之比，无法直接得知，需要更进一步估计。一种方法是简单谱减法，从功率角度$X_k^2= Y_k^2- \sigma_k^2$。 因此&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\xi_k = \gamma_k - 1&lt;/script&gt;

&lt;p&gt;更精确的方法是判决引导法（Decision-directed approach）， 滑动平均&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\xi_k = aX_{k-1}^2/\sigma_k^2+(1-a)\text{max}(\gamma_k - 1, 0)&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;参考文献&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[1]  P. C. Loizou, Speech enhancement: theory and practice. CRC press, 2007.&lt;/p&gt;
</description>
        <pubDate>Wed, 20 Nov 2019 01:00:00 +0800</pubDate>
        <link>https://chenwj1989.github.io/post/cn/monaural-speech-enhancement-review-cn.html</link>
        <guid isPermaLink="true">https://chenwj1989.github.io/post/cn/monaural-speech-enhancement-review-cn.html</guid>
        
        
        <category>语音</category>
        
      </item>
    
      <item>
        <title>单通道语音增强之参考书</title>
        <description>&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;推荐几本适合于学习单通道语音增强的参考书， 这几本书主要是针对传统信号处理方法，因为深度学习的方法是近几年才应用于语音增强，还在快速发展中，所以还没有成文的书籍。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;《Speech Enhancement: Theory and Practice》&lt;/em&gt;&lt;/strong&gt;，作者是Philips Loizou。如果说有哪本书是“学习传统单通道语音增强，看这一本就够了”，那就是这本了。 而且本书第一版有中文版，更是少有的语音增强中文资料。这本书覆盖了单通道语音增强传统方法的方方面面，包括谱减法、维纳滤波、子空间方法、统计信号分析、SPP估计、噪声估计、信噪比估计，该有的都有了。作者在第二版还探讨了传统方法的不足，作了一些Binary Gain方面的探索。可惜Loizou教授英年早逝，第二版已成为绝响了。&lt;/p&gt;

&lt;figure align=&quot;center&quot; style=&quot;width: 30%;margin:auto&quot;&gt;
  &lt;img width=&quot;100%&quot; height=&quot;auto&quot; src=&quot;/static/posts/2019/loizou.jpg&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;《Digital Speech Transmission: Enhancement, Coding and Error Concealment》&lt;/em&gt;&lt;/strong&gt;，作者是Peter Vary, Rainer Martin。这本书覆盖面更广些，除了单通道语音增强，还包括了音频编码、音频差错控制、多通道语音增强、回声消除，单通道语音增强方面也简单扼要地把各种信号处理方法介绍了一遍。这本书还是挺实用的，作为音频信号处理方面的参考书比较全面。&lt;/p&gt;

&lt;figure align=&quot;center&quot; style=&quot;width: 30%;margin:auto&quot;&gt;
  &lt;img width=&quot;100%&quot; height=&quot;auto&quot; src=&quot;/static/posts/2019/vary.jpg&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;《Sound Capture and Processing: Practical Approaches》&lt;/em&gt;&lt;/strong&gt;，作者Ivan J. Tashev现在是微软研究院做语音处理方向的专家。这本书大概是作者多年专业经验的总结，覆盖了音频采集、单通道去噪、麦克风阵列信号处理、声源定位、回声消除、去混响，所以特别适合研究语音前段信号处理的读者。另外在微软的官网可以看到作者项目组的研究方向，最新的的文章主要都是机器学习在语音信号处理方面的应用，也是很好的参考资料。&lt;/p&gt;

&lt;figure align=&quot;center&quot; style=&quot;width: 30%;margin:auto&quot;&gt;
  &lt;img width=&quot;100%&quot; height=&quot;auto&quot; src=&quot;/static/posts/2019/tashev.jpg&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;《Audio Source Separation and Speech Enhancement》&lt;/em&gt;&lt;/strong&gt;，由多位作者合编，是语音增强方面最新鲜的参考资料，2018年才刚刚出版。这本书涉及的话题很广，也有很多前沿的研究成果，主要都是围绕语音增强和语音源分离两个话题。 除了前面几本书所涉及到的单通道/多通道语音增强、声源定位、去混响之外， 这本书还介绍了大量的语音分离的信号处理和机器学习方法, 包括了聚类分析、独立成分分析（ICA）、非负矩阵分解（NMF）、高斯混合模型（GMM）等等。此书涉及的话题庞杂，而每章的篇幅有限，所以还需要和其他资料结合起来才能够理清细节。&lt;/p&gt;

&lt;figure align=&quot;center&quot; style=&quot;width: 30%;margin:auto&quot;&gt;
  &lt;img width=&quot;100%&quot; height=&quot;auto&quot; src=&quot;/static/posts/2019/gannot.jpg&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 07 Nov 2019 01:00:00 +0800</pubDate>
        <link>https://chenwj1989.github.io/post/cn/monaural-speech-enhancement-books-cn.html</link>
        <guid isPermaLink="true">https://chenwj1989.github.io/post/cn/monaural-speech-enhancement-books-cn.html</guid>
        
        
        <category>语音</category>
        
      </item>
    
      <item>
        <title>单通道语音增强之谱减法与维纳滤波</title>
        <description>&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#1-概述&quot; id=&quot;markdown-toc-1-概述&quot;&gt;1. 概述&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-谱减法-spectral-subtraction&quot; id=&quot;markdown-toc-2-谱减法-spectral-subtraction&quot;&gt;2. 谱减法 Spectral Subtraction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#31-维纳滤波-wiener-filtering&quot; id=&quot;markdown-toc-31-维纳滤波-wiener-filtering&quot;&gt;3.1 维纳滤波 Wiener Filtering&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;本文代码位于&lt;a href=&quot;https://github.com/chenwj1989/python-speech-enhancement&quot;&gt;GitHub&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&quot;1-概述&quot;&gt;1. 概述&lt;/h2&gt;
&lt;p&gt;单通道语音增强比较传统的方法是谱减法和维纳滤波， 在平稳噪声的条件下，能够获得较理想的去噪效果。 《语音增强–理论与实践》[@loizou2007speech]里面各有一章详细分析了这两种模型。&lt;/p&gt;

&lt;p&gt;假设麦克风采集到的带噪语音序列为$y[n]$，并且噪声都是加性噪声。则带噪语音序列为无噪语音序列与噪声序列的和。 原始语音信号与噪声均可视为随机信号。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y[n] = x[n] + d[n]&lt;/script&gt;

&lt;p&gt;常用的语音增强方法都是在频域，需要对带噪信号$y[n]$进行分帧、加窗、短时傅里叶变换（STFT）后，得到每一帧的频域信号，其中X，Y，D分别是干净语音、带噪信号和噪声的频域信号。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Y(\omega_{k}) = X(\omega_{k}) + D(\omega_{k})&lt;/script&gt;

&lt;p&gt;语音增强的目标是对实际信号$X(\omega_{k})$的幅度和相位进行估计。但是因为相位不易估计、而且研究表明相位对去噪效果影响比较小[@wang1982unimportance]，所以大部分方法都只对幅度谱进行增强，而相位则沿用带噪信号的相位。&lt;/p&gt;

&lt;p&gt;换句话说，语音增强就是要找出一个频域的实函数$H_{\omega_{k}}$, 并且将这个函数与带噪信号相乘，得到干净语音的估计。这个实函数称作抑制增益(Suppression Gain)。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{X}(\omega_{k}) = H(\omega_{k})|Y(\omega_{k})|e^{j\theta_{y}(k)}&lt;/script&gt;

&lt;p&gt;下面是单通道语音增强系统主要步骤的示意图，系统目标就是估计抑制增益，而抑制增益依赖于两个核心步骤：语音检测VAD和噪声估计模块。只有准确估计噪声谱$D(\omega_{k})$，才有可能准确估计抑制增益。 详细的VAD和噪声估计方法不在这篇文章里面详述，具体可以看参考文献。 一种简单的想法是先估计出VAD，如过判断此帧没有语音，则更新噪声谱，否则就沿用上一帧的噪声谱。&lt;/p&gt;

&lt;figure align=&quot;center&quot; style=&quot;width: 90%;margin:auto&quot;&gt;
  &lt;img width=&quot;100%&quot; height=&quot;auto&quot; src=&quot;/static/posts/ns_flow.png&quot; /&gt;
  &lt;figcaption&gt;Fig.1 - 单通道语音增强统计模型流程图&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;综上，语音增强的典型流程就是：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;对带噪语音y[n]分帧， 每一帧进行DFT得到$Y(\omega_{k})$。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;利用$Y(\omega_{k})$进行VAD检测和噪声估计。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;计算抑制增益$H_{k}$。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;抑制增益$H_{k}$与带噪信号谱相乘，得到纯净语音谱$\hat{X}(\omega_{k})$。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;对$\hat{X}(\omega_{k})$进行IDFT,得到纯净语音序列的估计$x[n]$。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;噪声估计模块可以估计噪声功率，也可以估计信噪比，避免信号幅度变化带来的误差。
定义后验信噪比：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\gamma_k = \frac{P_{yy}}{P_{dd}}&lt;/script&gt;

&lt;p&gt;定义先验信噪比：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\xi_k = \frac{P_{xx}}{P_{dd}}&lt;/script&gt;

&lt;h2 id=&quot;2-谱减法-spectral-subtraction&quot;&gt;2. 谱减法 Spectral Subtraction&lt;/h2&gt;

&lt;p&gt;谱减法是最直观的去噪声思想，就是带噪信号减去噪声的频谱，就等于干净信号的频谱。估计信号频谱的表达式如下，其中$\hat{D}(\omega_{k})$应是噪声估计模块得到的噪声频谱。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{X}(\omega_{k}) = Y(\omega_{k}) - \hat{D}(\omega_{k})&lt;/script&gt;

&lt;p&gt;传统的语音增强算法都不恢复准确的相位，而是沿用测量信号$Y(\omega_{k})$的相位，所以实际谱减法的公式为幅度谱相减：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{X}(\omega_{k}) = \left[|Y(\omega_{k})| - |\hat{D}(\omega_{k})|\right]e^{j\phi_y}&lt;/script&gt;

&lt;p&gt;其中$\hat{D}(\omega_{k})$是估计的噪声幅值。 最终增益函数为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(\omega_{k}) = \frac{|Y(\omega_{k})| - |\hat{D}(\omega_{k})|}{|Y(\omega_{k})|}&lt;/script&gt;

&lt;p&gt;从幅度谱角度做谱减，是架设了测量信号和噪声的相位一样，引入了较大失真。 从功率谱角度做谱减，是更常用的方法。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;|Y(\omega_{k})|^2 = |X(\omega_{k})|^2 + |D(\omega_{k})|^2 + X(\omega_{k})^*D(\omega_{k}) + X(\omega_{k})D(\omega_{k})^*&lt;/script&gt;

&lt;p&gt;假设语音信号与噪声不相关，于是得到估计的信号功率谱是测量信号功率谱减去估计的噪声功率谱。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;|X(\omega_{k})|^2 =  |Y(\omega_{k})|^2 -  |D(\omega_{k})|^2
P_{xx}(\omega_{k}) =  P_{yy}(\omega_{k}) -  P_{dd}(\omega_{k})&lt;/script&gt;

&lt;p&gt;因此抑制增益函数即为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(\omega_{k}) =  \sqrt{\frac{P_{yy}(\omega_{k}) -  P_{dd}(\omega_{k})}{P_{yy}(\omega_{k})}}&lt;/script&gt;

&lt;p&gt;用后验信噪比表示：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(\omega_{k}) = \sqrt{\frac{\gamma_k - 1}{\gamma_k}}&lt;/script&gt;

&lt;p&gt;用先验信噪比表示：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(\omega_{k}) = \sqrt{\frac{\xi_k}{\xi_k+1}}&lt;/script&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;spec_sub_gain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ksi'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;gain&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# gain function&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gain&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;31-维纳滤波-wiener-filtering&quot;&gt;3.1 维纳滤波 Wiener Filtering&lt;/h2&gt;

&lt;p&gt;维纳滤波的思想也很直接，就是将带噪信号经过线性滤波器变换来逼近原信号，并求均方误差最小时的线性滤波器参数。 维纳滤波可以是在时域推导，也可以在频域推导。如第一部分所介绍，语音增强系统通常在频域处理信号，所以下面只讨论频域维纳滤波。&lt;/p&gt;

&lt;figure align=&quot;center&quot; style=&quot;width: 90%;margin:auto&quot;&gt;
  &lt;img width=&quot;60%&quot; height=&quot;auto&quot; src=&quot;/static/posts/h_omega.png&quot; /&gt;
  &lt;figcaption&gt;Fig.2 - 线性滤波器进行语音增强&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;频域上，估计信号即为线性滤波器与输入信号的乘积。可见这个滤波器$H(\omega_{k})$正是我们要求的抑制增益函数。维纳滤波语音增强的目标就是寻找系数为实数的线性滤波器，使得滤波偶信号与原干净语音信号之间的均方误差最小。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{X}(\omega_{k}) = H(\omega_{k})Y(\omega_{k})&lt;/script&gt;

&lt;p&gt;均方误差为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E\left[|e(\omega_{k})|^2\right] = 
E\left\{\left[X(\omega_{k}) - H(\omega_{k})Y(\omega_{k})\right]\left[X(\omega_{k}) - H(\omega_{k})Y(\omega_{k})\right]^*\right\}&lt;/script&gt;

&lt;p&gt;这是一个优化问题，目标是求使得均方误差最小的参数$H$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(\omega_{k}) =\mathop{\arg\min}_{H(\omega_{k})} E\left[|e(\omega_{k})|^2\right]&lt;/script&gt;

&lt;p&gt;展开均方误差：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
E\left[|E(\omega_{k})|^2\right] = &amp;E\left[|X(\omega_{k})|^2\right] + |H(\omega_{k})|^2E\left[|Y(\omega_{k})|^2\right]\\&amp; -  H(\omega_{k})E\left[Y(\omega_{k})X(\omega_{k})^*\right] - H(\omega_{k})^*E\left[X(\omega_{k})Y(\omega_{k})^*\right]
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;对$H(\omega_{k})$求极值:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial E\left[|E(\omega_{k})|^2\right]}{\partial H(\omega_{k})} = H(\omega_{k})^*E\left[|Y(\omega_{k})|^2\right] -  E\left[Y(\omega_{k})X(\omega_{k})^*\right]&lt;/script&gt;

&lt;p&gt;得到极值点：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(\omega_{k})^* = \frac{E\left[Y(\omega_{k})X(\omega_{k})^*\right]}{E\left[|Y(\omega_{k})|^2\right]}&lt;/script&gt;

&lt;p&gt;求共轭后得到$H(\omega_{k})$：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(\omega_{k}) = \frac{E\left[X(\omega_{k})Y(\omega_{k})^*\right]}{E\left[|Y(\omega_{k})|^2\right]} 
= \frac{P_{xy}}{P_{yy}}&lt;/script&gt;

&lt;p&gt;$P_{xy}$是互功率，X与D不相关， $E\left[X(\omega_{k})D(\omega_{k})^*\right]=0$， 那么&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
P_{xy} &amp;= E\left\{X(\omega_{k})\left[X(\omega_{k})+D(\omega_{k})\right]^*\right\}\\
&amp;= E\left[|X(\omega_{k})|^2\right] + E\left[X(\omega_{k})D(\omega_{k})^*\right] \\
&amp;= P_{xx}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;带噪信号功率$P_{yy}$也可以展开为语音信号和噪声功率之和。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
P_{yy} &amp;= E\left\{\left[X(\omega_{k})+D(\omega_{k})\right]\left[X(\omega_{k})+D(\omega_{k})\right]^*\right\}\\
&amp;= E\left[|X(\omega_{k})|^2\right] +  E\left[|D(\omega_{k})|^2\right]  \\
&amp;= P_{xx} +  P_{dd}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;用先验信噪比表示&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(\omega_{k}) = \frac{P_{xx}}{P_{yy}} = \frac{P_{xx}}{P_{xx} +P_{dd} } = \frac{\xi_k}{\xi_k + 1 }&lt;/script&gt;

&lt;p&gt;用后验信噪比表示：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(\omega_{k}) = \frac{P_{xx}}{P_{yy}} = \frac{P_{yy}-P_{dd}}{P_{yy} } = \frac{1-\gamma_k}{\gamma_k }&lt;/script&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;wiener_gain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ksi'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;gain&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# gain function&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gain&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;参考文献&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[1]  P. C. Loizou, Speech enhancement: theory and practice. CRC press, 2007.&lt;/p&gt;

&lt;p&gt;[2]  D.Wang and J. Lim, The unimportance of phase in speech enhancement,” IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 30, no. 4, pp. 679-681, 1982.&lt;/p&gt;
</description>
        <pubDate>Thu, 31 Oct 2019 01:00:00 +0800</pubDate>
        <link>https://chenwj1989.github.io/post/cn/monaural-speech-enhancement-filtering-cn.html</link>
        <guid isPermaLink="true">https://chenwj1989.github.io/post/cn/monaural-speech-enhancement-filtering-cn.html</guid>
        
        
        <category>语音</category>
        
      </item>
    
      <item>
        <title>高斯混合模型与EM算法的推导</title>
        <description>&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#1-预备知识&quot; id=&quot;markdown-toc-1-预备知识&quot;&gt;1. 预备知识&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#11-高斯分布&quot; id=&quot;markdown-toc-11-高斯分布&quot;&gt;1.1 高斯分布&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#12-jensen不等式&quot; id=&quot;markdown-toc-12-jensen不等式&quot;&gt;1.2 Jensen不等式&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#13-矩阵求导&quot; id=&quot;markdown-toc-13-矩阵求导&quot;&gt;1.3 矩阵求导&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2高斯混合模型和em算法&quot; id=&quot;markdown-toc-2高斯混合模型和em算法&quot;&gt;2.高斯混合模型和EM算法&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#21-高斯混合模型gmm&quot; id=&quot;markdown-toc-21-高斯混合模型gmm&quot;&gt;2.1 高斯混合模型(GMM)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#22-em算法&quot; id=&quot;markdown-toc-22-em算法&quot;&gt;2.2 EM算法&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3em算法解单变量gmm&quot; id=&quot;markdown-toc-3em算法解单变量gmm&quot;&gt;3.EM算法解单变量GMM&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#31-e-step&quot; id=&quot;markdown-toc-31-e-step&quot;&gt;3.1 E-Step:&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#32-m-step&quot; id=&quot;markdown-toc-32-m-step&quot;&gt;3.2 M-Step:&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4em算法解多变量gmm&quot; id=&quot;markdown-toc-4em算法解多变量gmm&quot;&gt;4.EM算法解多变量GMM&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#41-e-step&quot; id=&quot;markdown-toc-41-e-step&quot;&gt;4.1 E-Step:&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#42-m-step&quot; id=&quot;markdown-toc-42-m-step&quot;&gt;4.2 M-Step:&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#5总结&quot; id=&quot;markdown-toc-5总结&quot;&gt;5.总结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1-预备知识&quot;&gt;1. 预备知识&lt;/h2&gt;

&lt;h3 id=&quot;11-高斯分布&quot;&gt;1.1 高斯分布&lt;/h3&gt;

&lt;p&gt;高斯分布是拟合随机数据最常用的模型。单变量$x$的高斯分布概率密函数如下:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textit{N}(x; \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right]&lt;/script&gt;

&lt;p&gt;其中&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$\mu $ 分布的数学期望,&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\sigma$ 标准差, $ \sigma ^{2}$ 是方差.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;更一般的情况，如果数据集是d维的数据, 就可以用多变量高斯模型来拟合。概率密度是:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textit{N}(x; \mu, \Sigma) = \frac{1}{\sqrt{(2\pi)^d\det(\Sigma)}}\exp\left[-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right]&lt;/script&gt;

&lt;p&gt;其中&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$x$是一个d×N的向量, 代表N组d维数据,&lt;/li&gt;
  &lt;li&gt;$\mu$是一个d×1 的向量, 代表每维的数学期望,&lt;/li&gt;
  &lt;li&gt;$\Sigma$是一个d×d的矩阵, 代表模型的协方差矩阵&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;12-jensen不等式&quot;&gt;1.2 Jensen不等式&lt;/h3&gt;

&lt;p&gt;这里给出随机分析里面Jensen’s不等式的结论。在EM算法的求解过程中，Jensen不等式可以简化目标函数。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;定理.&lt;/strong&gt;  对一个凸函数$f$和随机变量$x$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f\left[E(x)\right] \leq E\left[f(x)\right]&lt;/script&gt;

&lt;figure align=&quot;center&quot; style=&quot;width: 75%;margin:auto&quot;&gt;
  &lt;img width=&quot;55%&quot; height=&quot;auto&quot; src=&quot;/static/posts/convex.png&quot; /&gt;
  &lt;figcaption&gt;Fig.1 - 凸函数例子，设定$x$在a和b间均匀分布，$f(x)$的期望总比$f[E(x)]$大。&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;定理.&lt;/strong&gt;  对一个凹函数$f$和随机变量$x$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f\left[E(x)\right] \geq E\left[f(x)\right]&lt;/script&gt;

&lt;figure align=&quot;center&quot; style=&quot;width: 75%;margin:auto&quot;&gt;
  &lt;img width=&quot;55%&quot; height=&quot;auto&quot; src=&quot;/static/posts/concave.png&quot; /&gt;
  &lt;figcaption&gt;Fig.2 - 凹函数例子，设定$x$在a和b间均匀分布，$f(x)$的期望总比$f[E(x)]$小。&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;13-矩阵求导&quot;&gt;1.3 矩阵求导&lt;/h3&gt;

&lt;p&gt;多维高斯混合模型的求解需要借助于矩阵和向量求导的公式。
下面是从 《The Matrix Cookbook》一书中摘录在推导过程中可能会用到的公式。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial x^Ta}{\partial x} &amp;= \frac{\partial a^Tx}{\partial x} = a\\
\frac{\partial x^TBx}{\partial x} &amp;=  (B + B^T )x\\
\frac{\partial (x -s)^TW(x-s)}{\partial x} &amp;= -2W(x-s), \text{ (W是对称矩阵)} \\
\frac{\partial a^TXa}{\partial X} &amp;= \frac{\partial a^TX^Ta}{\partial X} = aa^T\\
\frac{\partial \det(X)}{\partial X} &amp;= \det(X)(X^{-1})^T\\
\frac{\partial \ln \det(X)}{\partial X} &amp;= (X^{-1})^T\\
\frac{\partial \det(X^{-1})}{\partial X} &amp;= -\det(X^{-1})(X^{-1})^T\\
\frac{\partial \ln \det(X)}{\partial X^{-1}} 
&amp;= \frac{\partial \ln \det(X)}{\partial \det(X)}\frac{\partial \det(X)}{\partial X^{-1}} \\
&amp;= \frac{1}{\det(X)}\left[-\det(X)X^T\right]\\
&amp;= -X^T\\
\frac{\partial Tr(AXB)}{\partial X} &amp;= A^TB^T\\
\frac{\partial Tr(AX^-1B)}{\partial X} &amp;= -(X^{-1}BAX^{-1})^T
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;2高斯混合模型和em算法&quot;&gt;2.高斯混合模型和EM算法&lt;/h2&gt;

&lt;h3 id=&quot;21-高斯混合模型gmm&quot;&gt;2.1 高斯混合模型(GMM)&lt;/h3&gt;

&lt;p&gt;现实采集的数据是比较复杂的，通常无法只用一个高斯分布拟合，而是可以看作多个随机过程的混合。可定义高斯混合模型是$K$个高斯分布的组合，用以拟合复杂数据。&lt;/p&gt;

&lt;p&gt;假设有一个数据集，包含了$N$个相互独立的数据：$x = {x_1, x_2 …x_i… x_N}$, 这些数据看起来有$K$个峰，这样的数据集可用以下定义的高斯混合模型拟合：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x|\Theta) = \sum_{k}^{}\alpha_{k}\textit{N}(x; \mu_k, \sigma_k) = \sum_{k}^{}\alpha_{k}\frac{1}{\sqrt{2\pi\sigma_k^2}}\exp\left[-\frac{(x-\mu_k)^2}{2\sigma_k^2}\right]&lt;/script&gt;

&lt;table style=&quot;width: 600px; height: 200px; margin:auto&quot;&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align:center&quot;&gt;
	&lt;img width=&quot;90%&quot; height=&quot;auto&quot; src=&quot;/static/posts/uniGMM.png&quot; /&gt;
	&lt;/td&gt;
        &lt;td style=&quot;text-align:center&quot;&gt;
	&lt;img width=&quot;90%&quot; height=&quot;auto&quot; src=&quot;/static/posts/multiGMM.png&quot; /&gt;
	&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align:center&quot; valign=&quot;top&quot;&gt;
  	&lt;figcaption&gt;Fig.3 - K=2的单变量GMM概率密度分布 &lt;/figcaption&gt;
	&lt;/td&gt;
        &lt;td style=&quot;text-align:center&quot; valign=&quot;top&quot;&gt;
  	&lt;figcaption&gt;Fig.4 -  K=2的双变量GMM例子&lt;/figcaption&gt;
	&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br /&gt;
如果每一个数据点$x_i$都是d维的, 这些数据$x$如上图看起来分散在$K$个聚类，这种数据集可以用多变量高斯混合模型拟合。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
p(x|\Theta) &amp;= \sum_{k}^{}\alpha_{k}\textit{N}(x; \mu_k, \Sigma_k)  \\
&amp;= \sum_{k}^{}\alpha_{k}\frac{1}{\sqrt{(2\pi)^d\det(\Sigma_k)}}\exp\left[-\frac{1}{2}(x-\mu_k)^T\Sigma_{k}^{-1}(x-\mu_k)\right]
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;其中$\Theta$ 代表全体高斯模型参数, $\alpha_k$ 是第$k$个高斯模型的先验概率, 各个高斯模型的先验概率加起来等于1。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{k}\alpha_k = 1&lt;/script&gt;

&lt;h3 id=&quot;22-em算法&quot;&gt;2.2 EM算法&lt;/h3&gt;
&lt;p&gt;EM 算法是一种迭代的算法，算法解决的问题可如下表述：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;采集到一组包含$N$个独立数据的数据集$x$。&lt;/li&gt;
  &lt;li&gt;预先知道、或者根据数据特点估计可以用$K$个高斯分布混合进行数据拟合。&lt;/li&gt;
  &lt;li&gt;目标任务是估计出高斯混合模型的参数：$K$组($\alpha_{k}$,
$\mu_k$, $\sigma_k$), 或 ($\alpha_{k}$, $\mu_k$, $\Sigma_{k}$).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;似然函数:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;对于相互独立的一组数据, 最大似然估计(MLE)是最直接的估计方法。$N$个数据点的总概率可以表述成每个数据点的概率之乘积，这被称作似然函数&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x|\Theta) = \prod_{i}p(x_i|\Theta)&lt;/script&gt;

&lt;p&gt;最大似然估计通过求似然函数的极大值，来估计参数$\Theta$。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Theta = \argmax_{\Theta} \prod_{i}p(x_i|\Theta)&lt;/script&gt;

&lt;p&gt;对高斯混合模型使用最大似然估计，求得的似然函数是比较的复杂的，单变量和多变量GMM似然函数结果如下，可以看到多变量GMM似然函数涉及多个矩阵的求逆和乘积等运算。所以要准确估计出$K$组高斯模型的参数，是很难的。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x|\Theta) = \prod_{i}p(x_i|\Theta) = \prod_{i}\left[\sum_{k}\alpha_{k}\textit{N}(x_i| \mu_k, \sigma_k) \right]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x|\Theta) = \prod_{i}p(x_i|\Theta) = \prod_{i}\left[\sum_{k}\alpha_{k}\textit{N}(x_i| \mu_k, \Sigma_k)\right]&lt;/script&gt;

&lt;p&gt;GMM 似然函数首先可以通过求对数进行简化，把乘积变成和。和的形式更方便求导和求极值。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(x|\Theta) = \sum_{i}\ln\left[p(x_{i}|\mu_k, \sigma_k) \right] = \sum_{i}\ln\left[\sum_{k}\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k) \right]&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;隐参数:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;是否对前面的对数似然函数进行求极大值，就可以求出目标的$K$组高斯模型参数了呢？我看到公式里面有两重求和，其中一重是在对数函数里面，直接求极值并不可行。&lt;/p&gt;

&lt;p&gt;EM算法提出了用迭代逼近的方法，来对最优的高斯混合模型进行逼近。为了帮助迭代算法的过程，EM算法提出了隐参数$z$, 每次迭代，先使用上一次的参数计算隐参数$z$的分布，然后使用$z$更新似然函数，对目标参数进行估计。
在GMM估计问题中，EM算法所设定的隐参量$z$ 一般属于${1 ,2 … k … K}$. 用于描述&lt;strong&gt;计算出GMM中$K$组高斯模型的参数后，某个数据点$x_i$属于第$z$个高斯模型的概率&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(z|x_{i}, \mu_k, \sigma_k)&lt;/script&gt;

&lt;p&gt;把隐参量$x$引入到第$i$个数据的概率估计中：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{i}|\Theta) = \sum_{k} p(x_{i}|z=k,\mu_k, \sigma_k) p(z=k) \\&lt;/script&gt;

&lt;p&gt;跟高斯混合分布 
$p(x|\Theta) = \sum_{k}^{}\alpha_{k}\textit{N}(x; \mu_k, \sigma_k)$ 作对比, 
发现$\alpha_k$就是$z$的先验分布$p(z=k)$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(z=k) = \alpha_k&lt;/script&gt;

&lt;p&gt;而在$z=k$条件下的$x$条件概率就是第$k$个高斯模型.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{i}|z=k,\mu_k, \sigma_k)  = \textit{N}(x_i; \mu_k, \sigma_k)&lt;/script&gt;

&lt;p&gt;现在可以把隐参量代入到对数似然函数中。可以加入冗余项：隐参数在数据$x_i$和高斯参数下的后验概率，从而引入Jensen不等式来简化似然函数。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
L(x|\Theta) &amp;= \sum_{i}\ln\left[p(x_{i}, z|\mu_k, \sigma_k) \right]  \\
&amp;= \sum_{i}\ln \sum_{k}  p(x_{i}|z=k, \mu_k, \sigma_k)p(z=k)   \\
&amp;= \sum_{i}\ln \sum_{k} p(z=k|x_{i},\mu_k, \sigma_k) \frac{p(x_{i}|z=k, \mu_k, \sigma_k)p(z=k)}{p(z=k|x_{i},\mu_k, \sigma_k)} \\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;似然函数简化:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;下面通过Jensen不等式简化对数似然函数。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f\left[E(x)\right] \geq E\left[f(x)\right]&lt;/script&gt;

&lt;p&gt;对照Jensen不等式，让$u$指代 
$\frac{p(x_{i}|z=k, \mu_k, \sigma_k)p(z=k)}{p(z|x_{i},\mu_k, \sigma_k)}$。&lt;/p&gt;

&lt;p&gt;可以得到&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(u) = \ln u&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E(u) = \sum_{k} p(z|x_{i},\mu_k, \sigma_k) u&lt;/script&gt;

&lt;p&gt;得到&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(x|\Theta) \geq \sum_{i}\sum_{k} p(z=k|x_{i},\mu_k, \sigma_k) \ln \frac{p(x_{i}|z=k, \mu_k, \sigma_k)p(z=k)}{p(z=k|x_{i},\mu_k, \sigma_k)}&lt;/script&gt;

&lt;p&gt;于是似然函数简化成对数函数的两重求和。等式右侧给似然函数提供了一个下界。&lt;/p&gt;

&lt;p&gt;我们可以根据贝叶斯准则进行推导其中的后验概率&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
p(z=k|x_{i},\mu_k, \sigma_k) &amp;= \frac{ p(x_{i}|z=k,\mu_k, \sigma_k)}{ \sum_{k} p(x_{i}|z=k, \mu_k, \sigma_k)}  \\
&amp;=  \frac{\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}{\sum_{k}\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)} 
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;定义&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\omega_{i,k} = p(z=k|x_{i},\mu_k, \sigma_k) = \frac{\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}{\sum_{k}\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}&lt;/script&gt;

&lt;p&gt;那么&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
L(x|\Theta) &amp;= \sum_{i}\ln \sum_{k} \omega_{i,k} \frac{\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}{\omega_{i,k}}  \\
&amp;\geq \sum_{i} \sum_{k} \omega_{i,k} \ln\frac{\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}{\omega_{i,k}} 
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;不等式的右侧给似然函数提供了一个下界。EM算法提出迭代逼近的方法，不断提高下界，从而逼近似然函数。每次迭代都以下面这个目标函数作为优化目标：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(\Theta,\Theta^{t}) = \sum_{i}\sum_{k}\omega_{i,k}^t \ln\frac{\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}{\omega_{i,k}^t}&lt;/script&gt;

&lt;p&gt;这个式子表示，在第$t$次迭代后，获得参数$\Theta^t$，然后就可以计算隐参数概率$\omega_{i,k}^t$。 将隐参数代回$Q(\Theta,\Theta^{t})$, 进行最大似然优化，即可求出更优的参数$\Theta^{t+1}$。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;迭代求解:&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;迭代开始时，算法先初始化一组参数值$\Theta$, 然后间隔地更新$\omega$和$\Theta$。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;经过$t$轮迭代,已获得一组目标参数$\Theta^t$临时的值。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;基于当前的参数$\Theta^t$，用高斯混合模型计算隐参数概率 $\omega_{i,k}^t$。然后将隐参数概率代入对数似然函数，得到似然函数数学期望表达式。
这一步叫&lt;strong&gt;expectation step&lt;/strong&gt;.&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\omega_{i,k}^t = \frac{\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}{\sum_{k}\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;如前文使用Jensen推导得出，得到每次更新了隐参数$\omega_{i,k}^t$后的目标函数是：&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(\Theta,\Theta^{t}) = \sum_{i}\sum_{k}\omega_{i,k}^t \ln\frac{\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}{\omega_{i,k}^t}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;利用$\omega_{i,k}$当前值, 最大化目标函数，从而得出新一组GMM参数 $\Theta^{t+1}$.  这一步叫作&lt;strong&gt;maximization step&lt;/strong&gt;。&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\Theta^{t+1} = \argmax_{\Theta} \sum_{i}\sum_{k} \ln \omega_{i,k}^t \frac{\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}{\omega_{i,k}^t}&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3em算法解单变量gmm&quot;&gt;3.EM算法解单变量GMM&lt;/h2&gt;

&lt;p&gt;单变量 GMM使用EM算法时，完整的目标函数为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(\Theta,\Theta^{t}) = \sum_{i}\sum_{k}\omega_{i,k}^t\ln\frac{\alpha_{k}}{\omega_{i,k}^t\sqrt{2\pi\sigma_k^2}}\exp\left[-\frac{(x_i-\mu_k)^2}{2\sigma_k^2}\right]&lt;/script&gt;

&lt;h3 id=&quot;31-e-step&quot;&gt;3.1 E-Step:&lt;/h3&gt;

&lt;p&gt;E-step目标就是计算隐参数的值， 也就是对每一个数据点，分别计算其属于每一种高斯模型的概率。 所以隐参量$\omega$是一个N×K矩阵.&lt;/p&gt;

&lt;p&gt;每一次迭代后 $\omega_{i,k}$都可以用最新的高斯参数$(\alpha_k, \mu_k, \sigma_k)$进行更新。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\omega_{i,k}^t = \frac{\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \sigma_k^t)}{\sum_{k}\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \sigma_k^t)}&lt;/script&gt;

&lt;p&gt;E-step 就可以把更新的$\omega$代入似然函数，得到目标函数的最新表达。该目标函数展开如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(\Theta,\Theta^{t}) = \sum_{i}\sum_{k}\omega_{i,k}^t\left(\ln\alpha_k - \ln \omega_{i,k}^t - \ln \sqrt{2\pi\sigma_k^2}-\frac{(x_i-\mu_k)^2}{2\sigma_k^2}\right)&lt;/script&gt;

&lt;h3 id=&quot;32-m-step&quot;&gt;3.2 M-Step:&lt;/h3&gt;

&lt;p&gt;M-step的任务就是最大化目标函数，从而求出高斯参数的估计。
&lt;script type=&quot;math/tex&quot;&gt;\Theta := \argmax_{\Theta} Q(\Theta,\Theta^{t})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;更新$\alpha_k:$&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在高斯混合模型定义中，$\alpha_k$受限于$\sum_{k}\alpha_k =1$。所以$\alpha_k$的估计是一个受限优化问题。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{gathered}
\alpha_k^{t+1} := \argmax_{\alpha_k}{ \sum_{i}\sum_{k}\omega_{i,k}^t\ln\alpha_k}\\
\text{subject to}  \sum_{k}\alpha_k =1 
\end{gathered}&lt;/script&gt;

&lt;p&gt;这种问题通常用拉格朗日乘子法计算。下面构造拉格朗日乘子：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}(\alpha_k, \lambda) = { \sum_{i}\sum_{k}\omega_{i,k}^t\ln\alpha_k}+ \lambda\left[\sum_{k}\alpha_k -1\right]&lt;/script&gt;

&lt;p&gt;对拉格朗日方程求极值，也就是对$\alpha_k$求导数为0处，该点就是我们要更新的$\alpha_{k}^{t+1}$值。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial \mathcal{L}(\alpha_k, \lambda) }{\partial \alpha_k} &amp;= { \sum_{i}\omega_{i,k}^t\frac{1}{\alpha_k}}+ \lambda = 0 \\
\Rightarrow  \alpha_k &amp;= -\frac{\sum_{i}\omega_{i,k}^t}{\lambda}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;将所有$k$项累加, 就可以求得$\lambda$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\sum_{k}\alpha_k &amp;= -\frac{\sum_{i}\sum_{k}\omega_{i,k}^t}{\lambda} \\
\Rightarrow 1 &amp;= -\sum_{i}\frac{1}{\lambda} = -\frac{N}{\lambda}  \\
\Rightarrow \lambda &amp;= -N  
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;于是利用地$t$次迭代的隐参量，我们就得到了$\alpha_k$在$t+1$次迭代的更新方程：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha_k^{t+1} = \frac{\sum_{i}\omega_{i,k}^t}{N}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;更新$\mu_k:$&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;$\mu_k$并没有类似$\alpha_k$的限制条件，可以直接把目标函数对$\mu_k$求导数：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_k^{t+1} :=  \argmax_{\mu_k}   Q(\Theta,\Theta^{t})&lt;/script&gt;

&lt;p&gt;让$\frac{\partial Q(\Theta,\Theta^{t})}{\partial \mu_k}=0$, 得到
&lt;script type=&quot;math/tex&quot;&gt;\frac{\partial \sum_{i}\sum_{k}\omega_{i,k}^t\left(\ln\alpha_k - \ln \omega_{i,k}^t - \ln \sqrt{2\pi\sigma_k^2}-\frac{(x_i-\mu_k)^2}{2\sigma_k^2}\right)}{\partial \mu_k} =  0&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\sum_{i}\omega_{i,k}^t\frac{x_i-\mu_k}{\sigma_k^2} = 0\\
\Rightarrow \sum_{i}\omega_{i,k}^t\mu_k = \sum_{i}\omega_{i,k}^tx_i \\
\Rightarrow \mu_k\sum_{i}\omega_{i,k}^t = \sum_{i}\omega_{i,k}^tx_i 
\end{aligned}&lt;/script&gt;

&lt;p&gt;所以在$t+1$次迭代， $\mu_k$就用全部$x$的加权平均来求得，权值正是$x_i$属于第$k$个模型产生的概率$\omega_{i,k}^t$。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_k^{t+1} = \frac{\sum_{i}\omega_{i,k}^tx_i}{\sum_{i}\omega_{i,k}^t}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;更新$\sigma_k:$&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;类似地, 将目标函数对$\sigma_k$求极大值：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma_k^{t+1} :=  \argmax_{\sigma_k}   Q(\Theta,\Theta^{t})&lt;/script&gt;

&lt;p&gt;让导数为0：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial Q(\Theta,\Theta^{t})}{\partial \sigma_k} = \frac{\partial \sum_{i}\sum_{k}\omega_{i,k}^t\left(\ln\alpha_k - \ln \omega_{i,k}^t - \ln \sqrt{2\pi\sigma_k^2}-\frac{(x_i-\mu_k)^2}{2\sigma_k^2}\right)}{\partial \sigma_k}=0&lt;/script&gt;

&lt;p&gt;得到&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
 \sum_{i}\omega_{i,k}\left[-\frac{1}{\sigma_k}+\frac{(x_i-\mu_k)^2}{\sigma_k^3}\right]&amp;= 0\\
\Rightarrow \sum_{i}\omega_{i,k}\sigma_k^2 &amp;= \sum_{i}\omega_{i,k}(x_i-\mu_k)^2 \\
\Rightarrow \sigma_k^2 \sum_{i}\omega_{i,k} &amp;= \sum_{i}\omega_{i,k}(x_i-\mu_k)^2 \\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;高斯模型里面使用的都是$\sigma_k^2$,所以就不需要求平方根了。$\sigma_k^2$的更新方程如下，依赖于更新的$\mu_k$。 所以一般都是先把$\mu_k^{t+1}$算出来，然后再更新$\sigma_k^2$。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(\sigma_k^2)^{t+1} = \frac{\sum_{i}\omega_{i,k}(x_i-\mu_k^{t+1})^2 }{\sum_{i}\omega_{i,k}}&lt;/script&gt;

&lt;h2 id=&quot;4em算法解多变量gmm&quot;&gt;4.EM算法解多变量GMM&lt;/h2&gt;

&lt;p&gt;同样的，我们可以得到每次迭代的目标函数如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(\Theta,\Theta^{t}) = \sum_{i}\sum_{k}\omega_{i,k}^t\ln\frac{\alpha_{k}}{\omega_{i,k}^t\sqrt{(2\pi)^d\det(\Sigma_k)}}\exp\left[-\frac{1}{2}(x_i-\mu_k)^T\Sigma_k^{-1}(x_i-\mu_k)\right]&lt;/script&gt;

&lt;p&gt;其中&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$x_i$是d×1的向量,&lt;/li&gt;
  &lt;li&gt;$\alpha_k$ 一个0和1间的值,&lt;/li&gt;
  &lt;li&gt;$\mu_k$是d×1的向量,&lt;/li&gt;
  &lt;li&gt;$\Sigma_k$是d×d的矩阵,&lt;/li&gt;
  &lt;li&gt;$\omega$是N×K的矩阵。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;41-e-step&quot;&gt;4.1 E-Step:&lt;/h3&gt;

&lt;p&gt;跟单变量GMM一样，E-step计算隐参数，但是需要用多维高斯分布，利用了多维矩阵乘法和矩阵求逆，计算复杂度要大很多。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\omega_{i,k}^t = \frac{\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \Sigma_k^t)}{\sum_{k}\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \Sigma_k^t)}&lt;/script&gt;

&lt;p&gt;目标函数更新如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp;Q(\Theta,\Theta^{t}) \\
&amp;= \sum_{i}\sum_{k}\omega_{i,k}^t\left(\ln\alpha_k - \ln \omega_{i,k}^t - \frac{d}{2}\ln \sqrt{(2\pi)^d} -\frac{1}{2}\ln\det(\Sigma_k)-\frac{1}{2}(x_i-\mu_k)^T\Sigma_k^{-1}(x_i-\mu_k)\right) 
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;42-m-step&quot;&gt;4.2 M-Step:&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;更新$\alpha_{k}:$&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;多变量GMM下，$\alpha_k$的更新跟单变量 GMM一样。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{gathered}
\alpha_k^{t+1} := \argmax_{\alpha_k}{ \sum_{i}\sum_{k}\omega_{i,k}^t\ln\alpha_k}\\
\text{subject to}  \sum_{k}\alpha_k =1 
\end{gathered}&lt;/script&gt;

&lt;p&gt;得到完全一样的更新方程：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha_k^{t+1} = \frac{\sum_{i}\omega_{i,k}^t}{N}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;更新$\mu_k:$&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_k^{t+1} :=  \argmax_{\mu_k}   Q(\Theta,\Theta^{t})&lt;/script&gt;

&lt;p&gt;$Q(\Theta,\Theta^{t})$对$\mu_k$求导，得到&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial Q(\Theta,\Theta^{t})}{\partial \mu_k} = \sum_{i}\omega_{i,k}^t\frac{\partial \left[-\frac{1}{2}(x_i-\mu_k)^T\Sigma_k^{-1}(x_i-\mu_k)\right]}{\partial \mu_k}  = 0\\&lt;/script&gt;

&lt;p&gt;实数协方差矩阵$\Sigma_{k}$对称的, 其逆矩阵也是对称的。 于是我们可以利用第一部分列出的公式$\frac{\partial (x -s)^TW(x-s)}{\partial x} = -2W(x-s)$求偏导数.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial Q(\Theta,\Theta^{t})}{\partial \mu_k} = 
 \sum_{i}\omega_{i,k}^t \Sigma_{k}^{-1}\left(x_i - \mu_k\right) =0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Rightarrow \sum_{i}\omega_{i,k}^t x_i= \mu_k\sum_{i}\omega_{i,k}^t&lt;/script&gt;

&lt;p&gt;所以$\mu_k$的更新方程同样是$x$的加权平均，只是这时候$\mu_k$ is a d×1 向量。
&lt;script type=&quot;math/tex&quot;&gt;\mu_k^{t+1} = \frac{\sum_{i}\omega_{i,k}^t x_i}{\sum_{i}\omega_{i,k}^t}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;更新$\Sigma_k:$&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Sigma_k^{t+1} :=  \argmax_{\Sigma_k}   Q(\Theta,\Theta^{t})&lt;/script&gt;

&lt;p&gt;让导数$\frac{\partial Q(\Theta,\Theta^{t})}{\partial \Sigma_k^{-1}} =0$, 得到&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial Q(\Theta,\Theta^{t})}{\partial \Sigma_k^{-1}} &amp;= \sum_{i}\omega_{i,k}^t\frac{\partial \left[  -\frac{1}{2}\ln\det(\Sigma_k)-\frac{1}{2}(x_i-\mu_k)^T\Sigma_k^{-1}(x_i-\mu_k)\right]}{\partial  \Sigma_k^{-1}}  \\
 &amp;= -\frac{1}{2} \sum_{i}\omega_{i,k}^t \left[\frac{\partial \ln\det(\Sigma_k)}{\partial  \Sigma_k^{-1}}+\frac{\partial (x_i-\mu_k)^T\Sigma_k^{-1}(x_i-\mu_k)}{\partial  \Sigma_k^{-1}} \right] \\
 &amp;= 0 
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;协方差矩阵$\Sigma_k$是对称的,可以利用第一部分的矩阵求导公式 $\frac{\partial \ln \det(X)}{\partial X^{-1}} =-X^T $ and $\frac{\partial a^TXa}{\partial X} = aa^T$，求得极大值$ Q(\Theta,\Theta^{t})$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial Q(\Theta,\Theta^{t})}{\partial \Sigma_k^{-1}} =   \frac{1}{2}\sum_{i}\omega_{i,k}^t \left[\Sigma_k - (x_i-\mu_k)(x_i-\mu_k)^T\right] = 0&lt;/script&gt;

&lt;p&gt;类似地, 我们可以得到$\Sigma_k$在第$t+1$次迭代的更新方程, 它依赖于$\mu_k$。所以我们需要先计算$\mu_k^{t+1}$，然后更新$\Sigma_k$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Sigma_k^{t+1} = \frac{\sum_{i}\omega_{i,k}^t (x_i-\mu_k^{t+1})(x_i-\mu_k^{t+1})^T }{\sum_{i}\omega_{i,k}^t}&lt;/script&gt;

&lt;h2 id=&quot;5总结&quot;&gt;5.总结&lt;/h2&gt;

&lt;table border=&quot;1&quot; style=&quot;width: 90%;margin:auto&quot;&gt;
    &lt;tr&gt;
        &lt;th style=&quot;text-align:center&quot;&gt;&lt;/th&gt;
        &lt;th style=&quot;text-align:center&quot;&gt;单变量GMM&lt;/th&gt;
        &lt;th style=&quot;text-align:center&quot;&gt;多变量GMM&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt; 
	&lt;th style=&quot;text-align:center&quot;&gt;初始化&lt;/th&gt;
        &lt;td&gt;$$\alpha_{k}^0, \mu_k^0, \sigma_k^0$$&lt;/td&gt;
        &lt;td&gt;$$\alpha_{k}^0, \mu_k^0, \Sigma_k^0$$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th style=&quot;text-align:center&quot;&gt;E-Step&lt;/th&gt;
        &lt;td&gt;$$
		\omega_{i,k}^t = \frac{\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \sigma_k^t)}{\sum_{k}\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \sigma_k^t)} 
		$$&lt;/td&gt;
        &lt;td&gt;$$
		\omega_{i,k}^t = \frac{\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \Sigma_k^t)}{\sum_{k}\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \Sigma_k^t)} 
		$$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th style=&quot;text-align:center&quot;&gt;M-Step&lt;/th&gt;
        &lt;td&gt;$$
		\begin{aligned} 
		\alpha_k^{t+1} &amp;amp;= \frac{\sum_{i}\omega_{i,k}^t}{N}\\ 
		\mu_k^{t+1} &amp;amp;= \frac{\sum_{i}\omega_{i,k}^t x_i}{\sum_{i}\omega_{i,k}^t}\\
		(\sigma_k^2)^{t+1} &amp;amp;= \frac{\sum_{i}\omega_{i,k}^t(x_i-\mu_k^{t+1})^2 }{\sum_{i}\omega_{i,k}^t}
		\end{aligned} 
		$$&lt;/td&gt;
        &lt;td&gt;$$\begin{aligned} 
		\alpha_k^{t+1} &amp;amp;= \frac{\sum_{i}\omega_{i,k}^t}{N}\\ 
		\mu_k^{t+1} &amp;amp;= \frac{\sum_{i}\omega_{i,k}^tx_i}{\sum_{i}\omega_{i,k}^t}\\
		\Sigma_k^{t+1} &amp;amp;= \frac{\sum_{i}\omega_{i,k}^t (x_i-\mu_k^{t+1})(x_i-\mu_k^{t+1})^T }{\sum_{i}\omega_{i,k}^t} 
		\end{aligned}$$&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

</description>
        <pubDate>Wed, 19 Jun 2019 14:32:04 +0800</pubDate>
        <link>https://chenwj1989.github.io/post/cn/gmm-em-cn.html</link>
        <guid isPermaLink="true">https://chenwj1989.github.io/post/cn/gmm-em-cn.html</guid>
        
        
        <category>机器学习</category>
        
      </item>
    
      <item>
        <title>Gaussian Mixture Model and  Expectation-Maximization Algorithm</title>
        <description>&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#1-preliminary-topics&quot; id=&quot;markdown-toc-1-preliminary-topics&quot;&gt;1. Preliminary Topics&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#11-gaussian-distribution&quot; id=&quot;markdown-toc-11-gaussian-distribution&quot;&gt;1.1 Gaussian Distribution&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#12-jensens-inequality&quot; id=&quot;markdown-toc-12-jensens-inequality&quot;&gt;1.2 Jensen’s Inequality&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#13-matrix-derivatives&quot; id=&quot;markdown-toc-13-matrix-derivatives&quot;&gt;1.3 Matrix Derivatives&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2gaussian-mixture-model-gmm-and-expectation-maximizationem-algorithm&quot; id=&quot;markdown-toc-2gaussian-mixture-model-gmm-and-expectation-maximizationem-algorithm&quot;&gt;2.Gaussian Mixture Model (GMM) and Expectation-Maximization(EM) Algorithm&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#21-gmm&quot; id=&quot;markdown-toc-21-gmm&quot;&gt;2.1 GMM&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#22-em&quot; id=&quot;markdown-toc-22-em&quot;&gt;2.2 EM&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3em-algorithm-for-univariate-gmm&quot; id=&quot;markdown-toc-3em-algorithm-for-univariate-gmm&quot;&gt;3.EM Algorithm for Univariate GMM&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#31-e-step&quot; id=&quot;markdown-toc-31-e-step&quot;&gt;3.1 E-Step:&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#32-m-step&quot; id=&quot;markdown-toc-32-m-step&quot;&gt;3.2 M-Step:&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4em-algorithm-for-multivariate-gmm&quot; id=&quot;markdown-toc-4em-algorithm-for-multivariate-gmm&quot;&gt;4.EM Algorithm for Multivariate GMM&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#41-e-step&quot; id=&quot;markdown-toc-41-e-step&quot;&gt;4.1 E-Step:&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#42-m-step&quot; id=&quot;markdown-toc-42-m-step&quot;&gt;4.2 M-Step:&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#5summary&quot; id=&quot;markdown-toc-5summary&quot;&gt;5.Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1-preliminary-topics&quot;&gt;1. Preliminary Topics&lt;/h2&gt;

&lt;h3 id=&quot;11-gaussian-distribution&quot;&gt;1.1 Gaussian Distribution&lt;/h3&gt;

&lt;p&gt;The Gaussian distribution is very widely used to fit random data. The
probability density for a one-dimensional random variable $x$ follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textit{N}(x; \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right]&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$\mu $ is the mean or expectation of the distribution,&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\sigma$ is the standard deviation, and $ \sigma ^{2}$ is the
variance.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More generally, when the data set is a d-dimensional data, it can be fit by a multivariate Gaussian model. The probability density is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textit{N}(x; \mu, \Sigma) = \frac{1}{\sqrt{(2\pi)^d\det(\Sigma)}}\exp\left[-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right]&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$x$ is a d-by-N vector, representing N sets of d-dimensional random data,&lt;/li&gt;
  &lt;li&gt;$\mu$ is a d-by-1 vector, representing the mean of each dimension,&lt;/li&gt;
  &lt;li&gt;$\Sigma$ is a d-by-d matrix, representing the covariance matrix&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;12-jensens-inequality&quot;&gt;1.2 Jensen’s Inequality&lt;/h3&gt;

&lt;p&gt;Here statements of Jensen’s inequality in the context of probability theory. These would be used to simplify the target function in an EM process.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt;For convex function $f$ and a random variable $x$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f\left[E(x)\right] \leq E\left[f(x)\right]&lt;/script&gt;

&lt;figure align=&quot;center&quot; style=&quot;width: 60%;margin:auto&quot;&gt;
  &lt;img width=&quot;70%&quot; height=&quot;70%&quot; src=&quot;/static/posts/convex.png&quot; /&gt;
  &lt;figcaption&gt;Fig.1 - An example of a convex function. Let $x$ be evenly distributed between
a and b. The expectation of $f(x)$ is always above $f[E(x)]$.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Theorem.&lt;/strong&gt; For concave function $f$ and a random variable $x$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f\left[E(x)\right] \geq E\left[f(x)\right]&lt;/script&gt;

&lt;figure align=&quot;center&quot; style=&quot;width: 60%;margin:auto&quot;&gt;
  &lt;img width=&quot;70%&quot; height=&quot;70%&quot; src=&quot;/static/posts/concave.png&quot; /&gt;
  &lt;figcaption&gt;Fig.2 - An example of a concave function. Let $x$ be evenly distributed
between a and b. The expectation of $f(x)$ is always under $f[E(x)]$.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;13-matrix-derivatives&quot;&gt;1.3 Matrix Derivatives&lt;/h3&gt;

&lt;p&gt;In order to solve the parameters in a Gaussian mixture model, we need
some rules about derivatives of a matrix or a vector. Here are some 
useful equations cited from &lt;em&gt;The Matrix Cookbook&lt;/em&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial x^Ta}{\partial x} &amp;= \frac{\partial a^Tx}{\partial x} = a\\
\frac{\partial x^TBx}{\partial x} &amp;=  (B + B^T )x\\
\frac{\partial (x -s)^TW(x-s)}{\partial x} &amp;= -2W(x-s), \text{ (W is symmetric)} \\
\frac{\partial a^TXa}{\partial X} &amp;= \frac{\partial a^TX^Ta}{\partial X} = aa^T\\
\frac{\partial \det(X)}{\partial X} &amp;= \det(X)(X^{-1})^T\\
\frac{\partial \ln \det(X)}{\partial X} &amp;= (X^{-1})^T\\
\frac{\partial \det(X^{-1})}{\partial X} &amp;= -\det(X^{-1})(X^{-1})^T\\
\frac{\partial \ln \det(X)}{\partial X^{-1}} 
&amp;= \frac{\partial \ln \det(X)}{\partial \det(X)}\frac{\partial \det(X)}{\partial X^{-1}} \\
&amp;= \frac{1}{\det(X)}\left[-\det(X)X^T\right]\\
&amp;= -X^T\\
\frac{\partial Tr(AXB)}{\partial X} &amp;= A^TB^T\\
\frac{\partial Tr(AX^-1B)}{\partial X} &amp;= -(X^{-1}BAX^{-1})^T\\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;2gaussian-mixture-model-gmm-and-expectation-maximizationem-algorithm&quot;&gt;2.Gaussian Mixture Model (GMM) and Expectation-Maximization(EM) Algorithm&lt;/h2&gt;

&lt;h3 id=&quot;21-gmm&quot;&gt;2.1 GMM&lt;/h3&gt;

&lt;p&gt;For a complex data set in the real-world, it normally consists of a
mixture of multiple stochastic processes. Therefore a single Gaussian
distribution cannot fit such data set. Instead, a Gaussian mixture model
is used to describe a combination of $K$ Gaussian distribution.&lt;/p&gt;

&lt;p&gt;Suppose we have a training set of $N$ independent data points $x = {x_1, x_2 …x_i… x_N}$, and the values show multiple peaks. We can model this data set by a Gaussian mixture model&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x|\Theta) = \sum_{k}^{}\alpha_{k}\textit{N}(x; \mu_k, \sigma_k) = \sum_{k}^{}\alpha_{k}\frac{1}{\sqrt{2\pi\sigma_k^2}}\exp\left[-\frac{(x-\mu_k)^2}{2\sigma_k^2}\right]&lt;/script&gt;

&lt;table style=&quot;width: 600px; height: 200px; margin:auto&quot;&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align:center&quot;&gt;
	&lt;img width=&quot;90%&quot; height=&quot;auto&quot; src=&quot;/static/posts/uniGMM.png&quot; /&gt;
	&lt;/td&gt;
        &lt;td style=&quot;text-align:center&quot;&gt;
	&lt;img width=&quot;90%&quot; height=&quot;auto&quot; src=&quot;/static/posts/multiGMM.png&quot; /&gt;
	&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align:center&quot; valign=&quot;top&quot;&gt;
  	&lt;figcaption&gt;Fig.3 - Probability density of a univariate GMM with K=2 &lt;/figcaption&gt;
	&lt;/td&gt;
        &lt;td style=&quot;text-align:center&quot; valign=&quot;top&quot;&gt;
  	&lt;figcaption&gt;Fig.4 - Samples of a 2d GMM with K=2&lt;/figcaption&gt;
	&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br /&gt;
When each data sample $x_i$ is d-dimensional, and the data set $x$ seem scattering to multiple clusters, the data can be modeled by a multivariate version Gaussian mixture model.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
p(x|\Theta) &amp;= \sum_{k}^{}\alpha_{k}\textit{N}(x; \mu_k, \Sigma_k)  \\
&amp;= \sum_{k}^{}\alpha_{k}\frac{1}{\sqrt{(2\pi)^d\det(\Sigma_k)}}\exp\left[-\frac{1}{2}(x-\mu_k)^T\Sigma_{k}^{-1}(x-\mu_k)\right]
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;In the models,  $\Theta$ means all parameters, and $\alpha_k$ is the prior probability of th $k^{th}$ Gaussian model, and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{k}\alpha_k = 1&lt;/script&gt;

&lt;h3 id=&quot;22-em&quot;&gt;2.2 EM&lt;/h3&gt;
&lt;p&gt;The expectation-maximization(EM) algorithm is an iterative supervised training algorithm. The task is formulated as:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;We have a training set of $N$ independent data points $x$.&lt;/li&gt;
  &lt;li&gt;Either we know or have a good guess, that the data set is a mixture
of $K$ Gaussian distributions.&lt;/li&gt;
  &lt;li&gt;The task is to estimate the GMM parameters: K set of ($\alpha_{k}$,
$\mu_k$, $\sigma_k$), or ($\alpha_{k}$, $\mu_k$, $\Sigma_{k}$).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Likelihood Function:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For estimation problems based on data set of independent samples, maximum-likelihood estimation (MLE) is a very widely used and straight forward method to perform estimation.&lt;/p&gt;

&lt;p&gt;The probability of N independent tests is described as the product of probability of each test. This is called the likelihood function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x|\Theta) = \prod_{i}p(x_i|\Theta)&lt;/script&gt;

&lt;p&gt;MLE is to estimate the  parameters $\Theta$ by maximizing the likelihood function.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Theta = \argmax_{\Theta} \prod_{i}p(x_i|\Theta)&lt;/script&gt;

&lt;p&gt;By applying the MLE , the likelihood function for uni and multiple variate Gaussian mixture models are very complicated.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x|\Theta) = \prod_{i}p(x_i|\Theta) = \prod_{i}\left[\sum_{k}\alpha_{k}\textit{N}(x_i| \mu_k, \sigma_k) \right]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x|\Theta) = \prod_{i}p(x_i|\Theta) = \prod_{i}\left[\sum_{k}\alpha_{k}\textit{N}(x_i| \mu_k, \Sigma_k)\right]&lt;/script&gt;

&lt;p&gt;To estimate K set of Gaussian parameters directly and explicitly is difficult. The EM algorithm simplifies the likelihood function of GMM, and provides an iterative way to optimize the estimation.Here we try to briefly describe the EM algorithm for GMM parameter estimation.&lt;/p&gt;

&lt;p&gt;First, the likelihood function of a GMM model can be simplified by taking the log likelihood function. An formula with the form of summation is easier for separating independent data samples and taking derivatives of parameters.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(x|\Theta) = \sum_{i}\ln\left[p(x_{i}|\mu_k, \sigma_k) \right] = \sum_{i}\ln\left[\sum_{k}\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k) \right]&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Latent Parameters:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There is no efficient way to explicitly maximizing the log likelihood function for GMM above. The EM algorithm introduces a latent parameter $z$, that $z \in {1 ,2 … k … K}$. That is used to describe the &lt;strong&gt;probability of a given training sample $x_i$ belonging to cluster z, given full GMM parameters&lt;/strong&gt;:
&lt;script type=&quot;math/tex&quot;&gt;p(z|x_{i}, \mu_k, \sigma_k)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Introduce the latent parameter $z$ in the  probability distribution of $x_i$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{i}|\Theta) = \sum_{k} p(x_{i}|z=k,\mu_k, \sigma_k) p(z=k) \\&lt;/script&gt;

&lt;p&gt;Compared with 
$p(x|\Theta) = \sum_{k}^{}\alpha_{k}\textit{N}(x|\mu_k, \sigma_k)$,&lt;/p&gt;

&lt;p&gt;we can conclude that $\alpha_k \text{ is the prior probability of } p(z=k)$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(z=k) = \alpha_k&lt;/script&gt;

&lt;p&gt;and the conditional probability of $x$ given $z=k$ is the $k^{th}$ Gaussian model.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{i}|z=k,\mu_k, \sigma_k)  = \textit{N}(x_i; \mu_k, \sigma_k)&lt;/script&gt;

&lt;p&gt;Now the latent parameter can be introduced into the log likelihood
function. Be noted that an redundant term $p(z|x_{i},\mu_k, \sigma_k) $
is added, in order to match the form of Jensen’s inequality.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
L(x|\Theta) &amp;= \sum_{i}\ln\left[p(x_{i}, z|\mu_k, \sigma_k) \right]  \\
&amp;= \sum_{i}\ln \sum_{k}  p(x_{i}|z=k, \mu_k, \sigma_k)p(z=k)   \\
&amp;= \sum_{i}\ln \sum_{k} p(z=k|x_{i},\mu_k, \sigma_k) \frac{p(x_{i}|z=k, \mu_k, \sigma_k)p(z=k)}{p(z=k|x_{i},\mu_k, \sigma_k)} \\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Simplify the Likelihood function:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;However the summation inside a log function make it difficult to maximize. Here recall Jensen’s inequality:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f\left[E(x)\right] \geq E\left[f(x)\right]&lt;/script&gt;

&lt;p&gt;Let $u$ represent 
$\frac{p(x_{i} | z=k, \mu_k, \sigma_k)p(z=k)}{p(z | x_{i},\mu_k, \sigma_k)}$ to match Jensen’s inequality.&lt;/p&gt;

&lt;p&gt;We get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(u) = \ln u&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E(u) = \sum_{k} p(z|x_{i},\mu_k, \sigma_k) u&lt;/script&gt;

&lt;p&gt;Therefore,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(x|\Theta) \geq \sum_{i}\sum_{k} p(z=k|x_{i},\mu_k, \sigma_k) \ln \frac{p(x_{i}|z=k, \mu_k, \sigma_k)p(z=k)}{p(z=k|x_{i},\mu_k, \sigma_k)}&lt;/script&gt;

&lt;p&gt;The posterior probability can be derived by the Bayes’ law.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
p(z=k|x_{i},\mu_k, \sigma_k) &amp;= \frac{ p(x_{i}|z=k,\mu_k, \sigma_k)}{ \sum_{k} p(x_{i}|z=k, \mu_k, \sigma_k)}  \\
&amp;=  \frac{\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}{\sum_{k}\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)} 
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Define
&lt;script type=&quot;math/tex&quot;&gt;\omega_{i,k} = p(z=k|x_{i},\mu_k, \sigma_k) = \frac{\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}{\sum_{k}\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
L(x|\Theta) &amp;= \sum_{i}\ln \sum_{k} \omega_{i,k} \frac{\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}{\omega_{i,k}}  \\
&amp;\geq \sum_{i} \sum_{k} \omega_{i,k} \ln\frac{\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}{\omega_{i,k}} 
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;This equation defines a lower bound for the log likelihood function. Therefore, an iterative target function for the EM algorithm is defined:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(\Theta,\Theta^{t}) = \sum_{i}\sum_{k}\omega_{i,k}^t \ln\frac{\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}{\omega_{i,k}^t}&lt;/script&gt;

&lt;p&gt;After $t$ iterations, we’ve got $\Theta^t$，and hence the latent $\omega_{i,k}^t$。 Apply the latest latent parameters in $Q(\Theta,\Theta^{t})$，and then we can update $\Theta^{t+1}$ by maiximization.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Iterative Optimization:&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;First the parameters $\Theta$ are initialized, and then $\omega$ and $\Theta$ are updated iteratively.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;After iteration t, a set of parameters $\Theta^t$ have been
achieved.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Calculate latent parameters $\omega_{i,k}^t$ by applying $\Theta^t$
into the GMM. This step is called &lt;strong&gt;expectation step&lt;/strong&gt;.&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\omega_{i,k}^t = \frac{\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}{\sum_{k}\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Apply the latest latent parameters $\omega_{i,k}^t$ in the target function. The target function is derived by simplifying the log likelihood funciton by Jensen’s inequality.&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(\Theta,\Theta^{t}) = \sum_{i}\sum_{k}\omega_{i,k}^t \ln\frac{\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}{\omega_{i,k}^t}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;With $\omega_{i,k}$, maximize the target log likelihood function, to
update GMM parameters $\Theta^{t+1}$. This step is called
&lt;strong&gt;maximization step&lt;/strong&gt;.&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\Theta^{t+1} = \argmax_{\Theta} \sum_{i}\sum_{k} \ln \omega_{i,k}^t \frac{\alpha_{k}\textit{N}(x_{i}| \mu_k, \sigma_k)}{\omega_{i,k}^t}&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3em-algorithm-for-univariate-gmm&quot;&gt;3.EM Algorithm for Univariate GMM&lt;/h2&gt;

&lt;p&gt;The complete form of the EM target function for a univariate GMM is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(\Theta,\Theta^{t}) = \sum_{i}\sum_{k}\omega_{i,k}^t\ln\frac{\alpha_{k}}{\omega_{i,k}^t\sqrt{2\pi\sigma_k^2}}\exp\left[-\frac{(x_i-\mu_k)^2}{2\sigma_k^2}\right]&lt;/script&gt;

&lt;h3 id=&quot;31-e-step&quot;&gt;3.1 E-Step:&lt;/h3&gt;

&lt;p&gt;The E-step is to estimate the latent parameters for each training sample on K Gaussian models. Hence the latent parameter $\omega$ is a N-by-K matrix.&lt;/p&gt;

&lt;p&gt;On every iteration, $\omega_{i,k}$ is calculated from the latest Gaussian parameters $(\alpha_k, \mu_k, \sigma_k)$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\omega_{i,k}^t = \frac{\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \sigma_k^t)}{\sum_{k}\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \sigma_k^t)}&lt;/script&gt;

&lt;h3 id=&quot;32-m-step&quot;&gt;3.2 M-Step:&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Theta := \argmax_{\Theta} Q(\Theta,\Theta^{t})&lt;/script&gt;

&lt;p&gt;The target likelihood function can be expanded to decouple items clearly.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(\Theta,\Theta^{t}) = \sum_{i}\sum_{k}\omega_{i,k}^t\left(\ln\alpha_k - \ln \omega_{i,k}^t - \ln \sqrt{2\pi\sigma_k^2}-\frac{(x_i-\mu_k)^2}{2\sigma_k^2}\right)&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Update $\alpha_k:$&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As defined in GMM, $\alpha_k$ is constrained by $\sum_{k}\alpha_k =1$, so estimating $\alpha_k$ is a constrained optimization problem.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{gathered}
\alpha_k^{t+1} := \argmax_{\alpha_k}{ \sum_{i}\sum_{k}\omega_{i,k}^t\ln\alpha_k}\\
\text{subject to}  \sum_{k}\alpha_k =1 
\end{gathered}&lt;/script&gt;

&lt;p&gt;The method of Lagrange multipliers is used to find the local maxima of such constrained optimization problem. We can construct a Lagrangean function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}(\alpha_k, \lambda) = { \sum_{i}\sum_{k}\omega_{i,k}^t\ln\alpha_k}+ \lambda\left[\sum_{k}\alpha_k -1\right]&lt;/script&gt;

&lt;p&gt;The local maxima $\alpha_{k}^{t+1}$ should make the derivative of the Lagrangean function equal to 0. Hence,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial \mathcal{L}(\alpha_k, \lambda) }{\partial \alpha_k} &amp;= { \sum_{i}\omega_{i,k}^t\frac{1}{\alpha_k}}+ \lambda = 0 \\
\Rightarrow  \alpha_k &amp;= -\frac{\sum_{i}\omega_{i,k}^t}{\lambda}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;By summing the equation for all $k$, the value of $\lambda$ can be calculated.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\sum_{k}\alpha_k &amp;= -\frac{\sum_{i}\sum_{k}\omega_{i,k}^t}{\lambda} \\
\Rightarrow 1 &amp;= -\sum_{i}\frac{1}{\lambda} = -\frac{N}{\lambda}  \\
\Rightarrow \lambda &amp;= -N  
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Therefore, $\alpha_k$ on iteration $t+1$ based on latent parameters on iteration $t$ is updated by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha_k^{t+1} = \frac{\sum_{i}\omega_{i,k}^t}{N}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Update $\mu_k:$&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;$\mu_k$ is unconstrained, and can be derived by taking the derivative of the target likelihood function.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_k^{t+1} :=  \argmax_{\mu_k}   Q(\Theta,\Theta^{t})&lt;/script&gt;

&lt;p&gt;Let $\frac{\partial Q(\Theta,\Theta^{t})}{\partial \mu_k}=0$, hence&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial \sum_{i}\sum_{k}\omega_{i,k}^t\left(\ln\alpha_k - \ln \omega_{i,k}^t - \ln \sqrt{2\pi\sigma_k^2}-\frac{(x_i-\mu_k)^2}{2\sigma_k^2}\right)}{\partial \mu_k} =  0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\sum_{i}\omega_{i,k}^t\frac{x_i-\mu_k}{\sigma_k^2} = 0\\
\Rightarrow \sum_{i}\omega_{i,k}^t\mu_k = \sum_{i}\omega_{i,k}^tx_i \\
\Rightarrow \mu_k\sum_{i}\omega_{i,k}^t = \sum_{i}\omega_{i,k}^tx_i 
\end{aligned}&lt;/script&gt;

&lt;p&gt;Hence $\mu_k$ on iteration $t+1$ can be updated as a form of weighted mean of $x$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_k^{t+1} = \frac{\sum_{i}\omega_{i,k}^tx_i}{\sum_{i}\omega_{i,k}^t}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Update $\sigma_k:$&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;Similarly, updated $\sigma_k$ is derived by taking the derivative of the target likelihood function with respect to $\sigma_k$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma_k^{t+1} :=  \argmax_{\sigma_k}   Q(\Theta,\Theta^{t})&lt;/script&gt;

&lt;p&gt;Let&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\frac{\partial Q(\Theta,\Theta^{t})}{\partial \sigma_k} = \frac{\partial \sum_{i}\sum_{k}\omega_{i,k}^t\left(\ln\alpha_k - \ln \omega_{i,k}^t - \ln \sqrt{2\pi\sigma_k^2}-\frac{(x_i-\mu_k)^2}{2\sigma_k^2}\right)}{\partial \sigma_k}=0&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;We get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
 \sum_{i}\omega_{i,k}\left[-\frac{1}{\sigma_k}+\frac{(x_i-\mu_k)^2}{\sigma_k^3}\right]&amp;= 0\\
\Rightarrow \sum_{i}\omega_{i,k}\sigma_k^2 &amp;= \sum_{i}\omega_{i,k}(x_i-\mu_k)^2 \\
\Rightarrow \sigma_k^2 \sum_{i}\omega_{i,k} &amp;= \sum_{i}\omega_{i,k}(x_i-\mu_k)^2 \\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;For $\sigma_k$, we can update $\sigma_k^2$, which is enough for Gaussian model calculation. New $sigma_k^2$ depends on $\mu_k$, so normally $\mu_k^{t+1}$ is calculated first and then applied to the update equation for $\sigma_k^2$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(\sigma_k^2)^{t+1} = \frac{\sum_{i}\omega_{i,k}(x_i-\mu_k^{t+1})^2 }{\sum_{i}\omega_{i,k}}&lt;/script&gt;

&lt;h2 id=&quot;4em-algorithm-for-multivariate-gmm&quot;&gt;4.EM Algorithm for Multivariate GMM&lt;/h2&gt;

&lt;p&gt;Similarlyt the target likelihood function for a multivariat GMM is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(\Theta,\Theta^{t}) = \sum_{i}\sum_{k}\omega_{i,k}^t\ln\frac{\alpha_{k}}{\omega_{i,k}^t\sqrt{(2\pi)^d\det(\Sigma_k)}}\exp\left[-\frac{1}{2}(x_i-\mu_k)^T\Sigma_k^{-1}(x_i-\mu_k)\right]&lt;/script&gt;

&lt;p&gt;Be aware that&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$x_i$ is a d-by-1 vecotr,&lt;/li&gt;
  &lt;li&gt;$\alpha_k$ is a real number between [0,1],&lt;/li&gt;
  &lt;li&gt;$\mu_k$ is a d-by-1 vector,&lt;/li&gt;
  &lt;li&gt;$\Sigma_k$ is a d-by-d matrix.&lt;/li&gt;
  &lt;li&gt;$\omega$ is a N-by-K matrix.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;41-e-step&quot;&gt;4.1 E-Step:&lt;/h3&gt;

&lt;p&gt;The E-step to estimate the latent parameters is the same as univariate GMM, except that the Gaussian distribution is a multivariate one, which is more complicated.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\omega_{i,k}^t = \frac{\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \Sigma_k^t)}{\sum_{k}\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \Sigma_k^t)}&lt;/script&gt;

&lt;p&gt;The target likelihood function can be expanded.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp;Q(\Theta,\Theta^{t}) \\
&amp;= \sum_{i}\sum_{k}\omega_{i,k}^t\left(\ln\alpha_k - \ln \omega_{i,k}^t - \frac{d}{2}\ln \sqrt{(2\pi)^d} -\frac{1}{2}\ln\det(\Sigma_k)-\frac{1}{2}(x_i-\mu_k)^T\Sigma_k^{-1}(x_i-\mu_k)\right) 
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;42-m-step&quot;&gt;4.2 M-Step:&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Update $\alpha_{k}:$&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The formula to update $\alpha_k$ for multivariate GMMs is exactly the same as univariate GMMs.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{gathered}
\alpha_k^{t+1} := \argmax_{\alpha_k}{ \sum_{i}\sum_{k}\omega_{i,k}^t\ln\alpha_k}\\
\text{subject to}  \sum_{k}\alpha_k =1 
\end{gathered}&lt;/script&gt;

&lt;p&gt;Hence we get the same update equation.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha_k^{t+1} = \frac{\sum_{i}\omega_{i,k}^t}{N}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Update $\mu_k:$&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_k^{t+1} :=  \argmax_{\mu_k}   Q(\Theta,\Theta^{t})&lt;/script&gt;

&lt;p&gt;Take the derivative of $Q(\Theta,\Theta^{t})$ with respec to $\mu_k$, we get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial Q(\Theta,\Theta^{t})}{\partial \mu_k} = \sum_{i}\omega_{i,k}^t\frac{\partial \left[-\frac{1}{2}(x_i-\mu_k)^T\Sigma_k^{-1}(x_i-\mu_k)\right]}{\partial \mu_k}  = 0\\&lt;/script&gt;

&lt;p&gt;As the covariance matrix $\Sigma_{k}$ is symmetric, the inverse of it is also symmetric.  We can apply $\frac{\partial (x -s)^TW(x-s)}{\partial x} = -2W(x-s)$ (see first section) to the partial derivative.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial Q(\Theta,\Theta^{t})}{\partial \mu_k} = 
 \sum_{i}\omega_{i,k}^t \Sigma_{k}^{-1}\left(x_i - \mu_k\right) =0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Rightarrow \sum_{i}\omega_{i,k}^t x_i= \mu_k\sum_{i}\omega_{i,k}^t&lt;/script&gt;

&lt;p&gt;Hence $\mu_k$ on iteration $t+1$ is also updated as a form of weighted mean of $x$. However, in this scenario $\mu_k$ is a d-by-1 vector.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_k^{t+1} = \frac{\sum_{i}\omega_{i,k}^t x_i}{\sum_{i}\omega_{i,k}^t}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Update $\Sigma_k:$&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Sigma_k^{t+1} :=  \argmax_{\Sigma_k}   Q(\Theta,\Theta^{t})&lt;/script&gt;

&lt;p&gt;Let $\frac{\partial Q(\Theta,\Theta^{t})}{\partial \Sigma_k^{-1}} =0$, we get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial Q(\Theta,\Theta^{t})}{\partial \Sigma_k^{-1}} &amp;= \sum_{i}\omega_{i,k}^t\frac{\partial \left[  -\frac{1}{2}\ln\det(\Sigma_k)-\frac{1}{2}(x_i-\mu_k)^T\Sigma_k^{-1}(x_i-\mu_k)\right]}{\partial  \Sigma_k^{-1}}  \\
 &amp;= -\frac{1}{2} \sum_{i}\omega_{i,k}^t \left[\frac{\partial \ln\det(\Sigma_k)}{\partial  \Sigma_k^{-1}}+\frac{\partial (x_i-\mu_k)^T\Sigma_k^{-1}(x_i-\mu_k)}{\partial  \Sigma_k^{-1}} \right] \\
 &amp;= 0 
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;By employing $\frac{\partial \ln \det(X)}{\partial X^{-1}} =-X^T $ and $\frac{\partial a^TXa}{\partial X} = aa^T$(see section one) for the symmetric covariance matrix $\Sigma_k$, and find the maxima of $ Q(\Theta,\Theta^{t})$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial Q(\Theta,\Theta^{t})}{\partial \Sigma_k^{-1}} =   \frac{1}{2}\sum_{i}\omega_{i,k}^t \left[\Sigma_k - (x_i-\mu_k)(x_i-\mu_k)^T\right] = 0&lt;/script&gt;

&lt;p&gt;Similarly, we get the update equation for  $\Sigma_k$ at iteration $t+1$, and it depends on $\mu_k$. So again $\mu_k^{t+1}$ is calculated first and then applied to the update equation for $\Sigma_k$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Sigma_k^{t+1} = \frac{\sum_{i}\omega_{i,k}^t (x_i-\mu_k^{t+1})(x_i-\mu_k^{t+1})^T }{\sum_{i}\omega_{i,k}^t}&lt;/script&gt;

&lt;h2 id=&quot;5summary&quot;&gt;5.Summary&lt;/h2&gt;

&lt;table border=&quot;1&quot; style=&quot;width: 90%;margin:auto&quot;&gt;
    &lt;tr&gt;
        &lt;th style=&quot;text-align:center&quot;&gt;&lt;/th&gt;
        &lt;th style=&quot;text-align:center&quot;&gt;Univariate GMM&lt;/th&gt;
        &lt;th style=&quot;text-align:center&quot;&gt;Multivariate GMM&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt; 
	&lt;th style=&quot;text-align:center&quot;&gt;Init&lt;/th&gt;
        &lt;td&gt;$$\alpha_{k}^0, \mu_k^0, \sigma_k^0$$&lt;/td&gt;
        &lt;td&gt;$$\alpha_{k}^0, \mu_k^0, \Sigma_k^0$$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th style=&quot;text-align:center&quot;&gt;E-Step&lt;/th&gt;
        &lt;td&gt;$$\omega_{i,k}^t = \frac{\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \sigma_k^t)}{\sum_{k}\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \sigma_k^t)}$$ &lt;/td&gt;
        &lt;td&gt;$$\omega_{i,k}^t = \frac{\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \Sigma_k^t)}{\sum_{k}\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \Sigma_k^t)}$$ &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th style=&quot;text-align:center&quot;&gt;M-Step&lt;/th&gt;
        &lt;td&gt;$$
		\begin{aligned} 
		\alpha_k^{t+1} &amp;amp;= \frac{\sum_{i}\omega_{i,k}^t}{N}\\ 
		\mu_k^{t+1} &amp;amp;= \frac{\sum_{i}\omega_{i,k}^t x_i}{\sum_{i}\omega_{i,k}^t}\\
		(\sigma_k^2)^{t+1} &amp;amp;= \frac{\sum_{i}\omega_{i,k}^t(x_i-\mu_k^{t+1})^2 }{\sum_{i}\omega_{i,k}^t}
		\end{aligned} 
		$$&lt;/td&gt;
        &lt;td&gt;$$
		\begin{aligned} 
		\alpha_k^{t+1} &amp;amp;= \frac{\sum_{i}\omega_{i,k}^t}{N}\\ 
		\mu_k^{t+1} &amp;amp;= \frac{\sum_{i}\omega_{i,k}^tx_i}{\sum_{i}\omega_{i,k}^t}\\
		\Sigma_k^{t+1} &amp;amp;= \frac{\sum_{i}\omega_{i,k}^t (x_i-\mu_k^{t+1})(x_i-\mu_k^{t+1})^T }{\sum_{i}\omega_{i,k}^t} 
		\end{aligned} 
		$$&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

</description>
        <pubDate>Tue, 18 Jun 2019 14:32:04 +0800</pubDate>
        <link>https://chenwj1989.github.io/post/en/gmm-em-en.html</link>
        <guid isPermaLink="true">https://chenwj1989.github.io/post/en/gmm-em-en.html</guid>
        
        
        <category>machine_learning</category>
        
      </item>
    
      <item>
        <title>单通道语音增强之统计信号模型</title>
        <description>&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#1-信号估计理论简述&quot; id=&quot;markdown-toc-1-信号估计理论简述&quot;&gt;1. 信号估计理论简述&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-最大似然估计ml&quot; id=&quot;markdown-toc-2-最大似然估计ml&quot;&gt;2. 最大似然估计ML&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3贝叶斯估计&quot; id=&quot;markdown-toc-3贝叶斯估计&quot;&gt;3.贝叶斯估计&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#31-最小均方估计mmse&quot; id=&quot;markdown-toc-31-最小均方估计mmse&quot;&gt;3.1 最小均方估计（MMSE）&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#311-mmse谱幅度估计&quot; id=&quot;markdown-toc-311-mmse谱幅度估计&quot;&gt;3.1.1 MMSE谱幅度估计&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#312-mmse对数谱幅度估计&quot; id=&quot;markdown-toc-312-mmse对数谱幅度估计&quot;&gt;3.1.2 MMSE对数谱幅度估计&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#313-mmse平方谱幅度估计&quot; id=&quot;markdown-toc-313-mmse平方谱幅度估计&quot;&gt;3.1.3 MMSE平方谱幅度估计&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#32-最大后验估计-map&quot; id=&quot;markdown-toc-32-最大后验估计-map&quot;&gt;3.2 最大后验估计 MAP&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#321-幅度和相位混合最大后验估计&quot; id=&quot;markdown-toc-321-幅度和相位混合最大后验估计&quot;&gt;3.2.1 幅度和相位混合最大后验估计&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#322-纯幅度最大后验估计&quot; id=&quot;markdown-toc-322-纯幅度最大后验估计&quot;&gt;3.2.2 纯幅度最大后验估计&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;本文代码位于&lt;a href=&quot;https://github.com/chenwj1989/python-speech-enhancement&quot;&gt;GitHub&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&quot;1-信号估计理论简述&quot;&gt;1. 信号估计理论简述&lt;/h2&gt;

&lt;p&gt;信号估计理论是现代统计处理的基础课题[@ZhangXianDa2002ModernSP]，在通信、语音、图像领域均有广泛应用。语音增强，就是从带噪的语音测量信号中估计原始的无噪语音，这是典型的信号估计问题。
《语音增强–理论与实践》[@loizou2007speech]一书中列举了用于语音增强的一系列统计模型。&lt;/p&gt;

&lt;p&gt;假设麦克风采集到的带噪语音序列为&lt;script type=&quot;math/tex&quot;&gt;y[n]&lt;/script&gt;，并且噪声都是加性噪声。则带噪语音序列为无噪语音序列与噪声序列的和。原始语音信号与噪声均可视为随机信号。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y[n] = x[n] + d[n]&lt;/script&gt;

&lt;p&gt;在时域对$x[n]$进行估计是非常困难的，通过傅立叶变换，我们可以将信号分解为频域上互相独立的系数。信号估计模型转变为对每一个频点的系数进行估计的模型，不同频点之间的参数是相互独立的。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Y(\omega_{k}) = X(\omega_{k}) + D(\omega_{k})&lt;/script&gt;

&lt;p&gt;这个方法就叫做统计信号谱分析（Statistical Spectral
Analysis)。显然地，纯净信号谱$X(\omega_{k})$带有幅度与相位两参数，我们实际上是对幅度$X_{k}$和相位$\theta_{y}(k)$进行参数估计。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Y_{k}e^{j\theta_{y}(k)} = X_{k}e^{j\theta_{x}(k)} + D_{k}e^{j\theta_{d}(k)}&lt;/script&gt;

&lt;p&gt;重组谱幅度和谱相位估计值即可恢复纯净语音谱，估计值用上标来表示：$\hat{X}(\omega_{k})=\hat{X}(k)e^{j\hat{\theta}_{x}(k)}$。&lt;/p&gt;

&lt;p&gt;实际信号的幅度和相位是不方便直接用在运算过程中的，因为信号取值范围不定，且瞬时变化。在噪声抑制领域，更常用的语音谱估计方法是对抑制增益(Suppression
Gain)进行估计，不同的估计准则称为抑制准则(Supression Rule)。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{X}(\omega_{k}) = H_{k}Y(\omega_{k})&lt;/script&gt;

&lt;p&gt;通常会根据先验信噪比、后验信噪比来估计抑制增益$H_{k}$。
并且可以在只有噪声出现的时刻更新$H_{k}$，
在语音存在的时刻进行抑制，无须每帧去调用噪声抑制算法，计算过程比直接估计信号谱灵活。&lt;/p&gt;

&lt;p&gt;综上，语音增强的典型流程就是：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;对带噪语音y[n]分帧， 每一帧进行DFT得到$Y(\omega_{k})$。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;估计或者沿用上一帧的抑制增益$H_{k}$，得到纯净语音谱$\hat{X}(\omega_{k})$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;对$\hat{X}(\omega_{k})$进行IDFT,得到纯净语音序列的估计$x[n]$。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;为了估计模型的建模，对测量信号、估计信号、噪声信号都需要作一些数学上的假设和简化。其中对噪声一般会作以下假设：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;噪声是与语音独立的加性噪声；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;每一帧噪声的统计分布是稳态的；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;噪声的傅立叶级数是零均值复高斯分布。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left[D(\omega_{k})\right] = 
\frac{1}{\pi\lambda_{d}(k)}
\exp\left[-\frac{|D(\omega_{k})|^2}{\lambda_{d}(k)}\right]&lt;/script&gt;

&lt;h2 id=&quot;2-最大似然估计ml&quot;&gt;2. 最大似然估计ML&lt;/h2&gt;

&lt;p&gt;如果不考虑信号的先验分布，即认为信号值是确定信号，而不是随机信号，我们只需要分析含有信号$x$为参数的带噪信号$y$的概率分布$p(y;x)$，并使之最大。这种估计方法叫最大似然估计器（Maximum
Likelihood
Estimation）。将$y$的带参概率分布$p(y;x)$称为似然函数（Likelihood
Function）。对纯净信号$x$的估计，表达为求解合适的$x$值，使得似然函数$p(y;x)$最大。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{x} = \argmax_{x}p(y;x)&lt;/script&gt;

&lt;p&gt;文献[@mcaulay1980speech]最早将最大似然估计法用在语音增强领域。对于纯净语音，可以假设纯净语音幅度$X_{k}$和相位$\theta_{y}(k)$是未知但确定，无需考虑其先验概率分布。最大似然语音增强模型表达为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{X}_{k},\hat{\theta}_{k} = \argmax_{X_{k},\theta_{k}} p\left[Y(\omega_{k});X_{k},\theta_{k}\right]&lt;/script&gt;

&lt;p&gt;把$D_{k}e^{\theta_{d}(k)}=Y_{k}e^{\theta_{y}(k)} - X_{k}e^{\theta_{x}(k)} $代入噪声零均值复高斯分布公式中，得到：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p\left[Y(\omega_{k});X_{k},\theta_{k}\right] = \frac{1}{\pi \lambda_{d}(k)}\exp\left[-\frac{|Y(\omega_{k}) - X_{k}e^{j\theta_{x}(k)}|^2}{\lambda_{d}(k)}\right]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H_{k} = \frac{1}{2}+\frac{1}{2}\sqrt{\frac{\xi_{k}}{1+\xi_{k}}}&lt;/script&gt;

&lt;h2 id=&quot;3贝叶斯估计&quot;&gt;3.贝叶斯估计&lt;/h2&gt;

&lt;p&gt;如果比最大似然估计更进一步，考虑待估计量$x$也是随机变量，且$x$的先验分布为$p(x)$，这种假设下的估计方法叫做贝叶斯估计[@ZhangXianDa2002ModernSP]。定义估计值$\hat{x}$与实际值$x$之间的误差函数为$c(\hat{x},x)$，贝叶斯估计器的目标即为找出是平均误差$E[c(\hat{x},x)]$最小的估计值$x$。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{x} = \argmin_{\hat{x}} E[c(\hat{x}, x)]&lt;/script&gt;

&lt;p&gt;对于待估计的纯净语音谱，贝叶斯估计器可以表达为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{X}(\omega_{k}) = \argmin_{\hat{X_{k}}} E[c(\hat{X}(\omega_{k}), X(\omega_{k}))]&lt;/script&gt;

&lt;p&gt;误差$c(\hat{x},x)$的期望值取决于待测信号与测量信号的联合概率分布。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[c(\hat{x}, x)] = \int_{x}\int_{y}c(\hat{x}, x)p(x, y)dxdy&lt;/script&gt;

&lt;p&gt;根据条件概率密度公式对联合概率密度进行分解：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[c(\hat{x}, x)]  = \int_{x}\int_{y}c(\hat{x}, x)p(x|y)p(y)dxdy = \int_{y} \left[ \int_{x}c(\hat{x}, x)p(x|y)dx\right] p(y)dy&lt;/script&gt;

&lt;p&gt;因为估计值$\hat{x}$与$p(y)$相互独立，可以把外层对$y$的积分消除，也就是：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{x} = \argmin_{\hat{x}} E[c(\hat{x}, x)|y]&lt;/script&gt;

&lt;p&gt;误差函数$c(\hat{x}, x)$的计算模型，会引出不同种类的估计器。典型的误差函数有几种类型[@ZhangXianDa2002ModernSP]：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;平方误差函数，对应最小均方估计。&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;c(\hat{x}, x)  = (\hat{x} - x)^2&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;绝对值误差函数，对应条件中位数估计。&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;c(\hat{x}, x)  = |\hat{x} - x|&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;均匀误差函数，对应最大后验估计。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
c(\hat{x}, x)   =
            \left\{
            \begin{array}{lr}
            1 \quad (|\hat{x}-x|\geq\Delta/2) &amp;  \\
            &amp;\Delta&gt;0\\
            0 \quad (|\hat{x}-x|&lt;\Delta/2) &amp; 
            \end{array}
            \right. %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;31-最小均方估计mmse&quot;&gt;3.1 最小均方估计（MMSE）&lt;/h3&gt;

&lt;p&gt;最小均方估计(Minimum Mean Square Error
Estimation)[@ephraim1985speech]使用平方误差函数，平均误差为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[c(\hat{x}, x)|y]=\int_{x}(\hat{x} - x)^2 p(x|y)dx&lt;/script&gt;

&lt;p&gt;为求平均误差的极大值，可对估计量$\hat{x}$求导，并求极值点。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial E[c(\hat{x}, x)|y]}{\partial \hat{x}}=\int_{x}2(\hat{x} - x) p(x|y)dx =0&lt;/script&gt;

&lt;p&gt;显然极值点的$\hat{x}$为被估计量$x$的条件均值：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{x} =\int_{x}x p(x|y)dx = E[ x| y]&lt;/script&gt;

&lt;p&gt;亦可以使用贝叶斯公式展开，得到：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    \hat{x} = \int_{x}x \frac{p(y|x)p(x)}{p(y)} dx \\
            = \frac{\int_{x}xp(y|x)p(x)dx}{\int_{x}p(y|x)p(x)dx}
\end{aligned}&lt;/script&gt;

&lt;p&gt;在语音增强的模型里，纯净语音谱的估计为其在带噪语音谱下的条件均值。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{X}(\omega_{k}) = E[X(\omega_{k})| Y(\omega_{k})]&lt;/script&gt;

&lt;p&gt;上式中，为得到纯净语音谱需要分别估计谱幅度和谱相位: $\hat{X}, \hat{\theta}_{x}$。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{X}_{k}e^{j\hat{\theta}_{x}(k)} = E[ X_{k}e^{j\theta_{x}(k)}| Y(\omega_{k})]&lt;/script&gt;

&lt;p&gt;同时估计谱幅度和谱相位是很难的，研究者提出了许多分别估计谱幅度和谱相位的方法，估计完成后再用两者重组复语音信号。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{X}_{k} = E[ X_{k}| Y(\omega_{k})]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;e^{j\hat{\theta}_{x}(k)} = E[ e^{j\theta_{x}(k)}| Y(\omega_{k})]&lt;/script&gt;

&lt;h4 id=&quot;311-mmse谱幅度估计&quot;&gt;3.1.1 MMSE谱幅度估计&lt;/h4&gt;

&lt;p&gt;最小均方根估计器MMSE short-time spectral amplitude&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{X_{k}} = E[ X_{k}| Y(\omega_{k})]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_{k} = \frac{\xi_{k}}{1+\xi_{k}}\gamma_{k}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H_{k} =\frac{\sqrt{\pi v_{k}}}{ 2\gamma_{k}}[(1+v_{k})I_{0}(\frac{v_k}{2})+v_{1}I_{1}(\frac{v_{k}}{2})]\exp(\frac{-v_{k}}{2})&lt;/script&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mmse_stsa_gain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gamma'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ksi'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
			
		&lt;span class=&quot;n&quot;&gt;vk&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;j0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vk&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;j1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vk&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
			
		&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vk&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j1&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;gain&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gain&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;312-mmse对数谱幅度估计&quot;&gt;3.1.2 MMSE对数谱幅度估计&lt;/h4&gt;

&lt;p&gt;对数最小均方根估计器The MMSE log spectral amplitude (MMSE-LSA)，
或者缩写为LogMMSE估计器。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;c(\hat{X_{k}}, X_{k})  = (\log{X_{k}}- \log{X_{k}})^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\log{\hat{X_{k}}} = E[ \log{X_{k}}| Y(\omega_{k})]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H_{k} = \frac{\xi_{k}}{1+\xi_{k}}\exp(\frac{1}{2}\int_{v_{k}}^{\infty}\frac{e^{-t}}{t}dt)&lt;/script&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;logmmse_gain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gamma'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ksi'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;vk&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;ei_vk&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;expn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;gain&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ei_vk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gain&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;313-mmse平方谱幅度估计&quot;&gt;3.1.3 MMSE平方谱幅度估计&lt;/h4&gt;

&lt;p&gt;频谱幅度平方估计器MMSE magnitude squared[@wolfe2003efficient]&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{X_{k}^2} = E[ X_{k}^2| Y_{k}]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{X_{k}^2} = \frac{\xi_{k}}{1+\xi_{k}} (\frac{1+v_{k}}{\gamma_{k}})Y_{k}^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H_{k} = \sqrt{\frac{\xi_{k}}{1+\xi_{k}} (\frac{1+v_{k}}{\gamma_{k}})}&lt;/script&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mmse_sqr_gain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gamma'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ksi'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
			
		&lt;span class=&quot;n&quot;&gt;vk&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;j0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vk&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;j1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vk&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
			
		&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;gain&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gain&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;32-最大后验估计-map&quot;&gt;3.2 最大后验估计 MAP&lt;/h3&gt;

&lt;p&gt;当贝叶斯估计器采用均匀误差函数时，平均误差为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
    E[c(\hat{x}, x)|y] &amp;= \int_{-\infty}^{\hat{x}-\Delta/2}  p(x|y)dx + \int_{\hat{x}+\Delta/2}^{\infty}  p(x|y)dx   \\
                       &amp;= 1-\int_{\hat{x}\Delta/2}^{\hat{x}+\Delta/2}  p(x|y)dx  
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;显然要使得平均误差最小，就是要求目标估计$\hat{x}$，使得
$p(x|y)$最大。这种估计模型称作最大后验估计(Maximum A Posteriori
Estimation)。这个模型的意思是只有估计值$\hat{x}$与原始值$x$相等，误差才为0，其他时候误差均匀为1。估计值可以表达为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{x} =\argmin_{x} p(x|y)&lt;/script&gt;

&lt;p&gt;在语音增强的模型里，纯净语音谱的估计为其在带噪语音谱下的条件均值。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{X}(\omega_{k}) = \argmin_{ X(\omega_{k})}\quad p\left[X(\omega_{k})| Y(\omega_{k})\right]&lt;/script&gt;

&lt;p&gt;文献[@wolfe2003efficient]提出了两种基于最大后验估计(Maximum A Posteriori
Estimation)的语音增强算法。一种是同时求解幅度和相位的混合最大后验估计：
另外一种是单纯估计幅度的方法，两种估计器最后的噪声抑制增益略有不同。&lt;/p&gt;

&lt;h4 id=&quot;321-幅度和相位混合最大后验估计&quot;&gt;3.2.1 幅度和相位混合最大后验估计&lt;/h4&gt;

&lt;p&gt;[@wolfe2003efficient]&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H_{k} = \frac{\xi_{k}+\sqrt{\xi_{k}^2+2(1+\xi_{k})\frac{\xi_{k}}{\gamma_{k}}}}{2(1+\xi_{k})}&lt;/script&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;map_joint_gain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gamma'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ksi'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
	
	&lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-6&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;gain&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gain&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;322-纯幅度最大后验估计&quot;&gt;3.2.2 纯幅度最大后验估计&lt;/h4&gt;

&lt;p&gt;[@wolfe2003efficient]&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H_{k} = \frac{\xi_{k}+\sqrt{\xi_{k}^2+(1+\xi_{k})\frac{\xi_{k}}{\gamma_{k}}}}{2(1+\xi_{k})}&lt;/script&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;map_sa_gain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gamma'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ksi'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
	
	&lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-6&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;gain&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ksi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gain&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参考文献&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[1] 张贤达, 现代信号处理. 清华大学出版社有限公司, 2002.&lt;/p&gt;

&lt;p&gt;[2] P. C. Loizou, Speech enhancement: theory and practice. CRC press, 2007.&lt;/p&gt;

&lt;p&gt;[3] R. McAulay and M. Malpass, Speech enhancement using a soft-decision noise suppression filter,” IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 28, no. 2, pp. 137-145,1980.&lt;/p&gt;

&lt;p&gt;[4] Y. Ephraim and D. Malah, Speech enhancement using a minimum mean-square error log-spectral amplitude estimator,” IEEE transactions on acoustics, speech, and signal processing,vol. 33, no. 2, pp. 443-445, 1985.&lt;/p&gt;

&lt;p&gt;[5] P. J. Wolfe and S. J. Godsill, Efficient alternatives to the ephraim and malah suppression rule for audio signal enhancement,” EURASIP Journal on Applied Signal Processing, vol. 2003, pp.1043-1051, 2003.&lt;/p&gt;
</description>
        <pubDate>Tue, 28 May 2019 15:32:04 +0800</pubDate>
        <link>https://chenwj1989.github.io/post/cn/monaural-speech-enhancement-statistical-cn.html</link>
        <guid isPermaLink="true">https://chenwj1989.github.io/post/cn/monaural-speech-enhancement-statistical-cn.html</guid>
        
        
        <category>语音</category>
        
      </item>
    
  </channel>
</rss>
